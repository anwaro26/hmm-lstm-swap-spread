{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "try:\n",
    "    PATH_TO_EXCEL  \n",
    "except NameError:\n",
    "    PATH_TO_EXCEL = \"/Users/anwarouni/Downloads/Thesis/Code/output_data.xlsx\"\n",
    "\n",
    "DATE_COL   = \"Date\"\n",
    "TARGET_COL = \"Swap Spread\"\n",
    "\n",
    "# Your exogenous features (fixed; no selection)\n",
    "EXOGS = [\"VSTOXX\",\"Euribor-OIS\",\"Yield_Slope\",\"Credit_Risk\"]\n",
    "\n",
    "# Folds and horizons\n",
    "SPLITS   = [(0.60,0.70),(0.70,0.80),(0.80,0.90),(0.90,1.00)]\n",
    "HORIZONS = [1,4,12]\n",
    "\n",
    "# Small grid (expand if you like)\n",
    "L_GRID     = [12, 24, 52]\n",
    "HID_GRID   = [64, 128]\n",
    "DROPOUTS   = [0.0, 0.2]\n",
    "\n",
    "# Training\n",
    "VAL_FRAC     = 0.15           \n",
    "BATCH        = 64\n",
    "EPOCHS_FULL  = 120\n",
    "PATIENCE     = 15\n",
    "LR           = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP_NORM    = 1.0\n",
    "SEED         = 123\n",
    "EPS          = 1e-8           \n",
    "\n",
    "# Repro/Device\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- Data load --------------------\n",
    "df = pd.read_excel(PATH_TO_EXCEL, engine=\"openpyxl\")\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "for c in [TARGET_COL] + EXOGS:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Δy and rolling std\n",
    "df[\"Δy\"] = df[TARGET_COL].diff()\n",
    "df[\"std12_dy\"] = df[\"Δy\"].rolling(12, min_periods=6).std()\n",
    "\n",
    "dates = df[DATE_COL].values\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def setup_dates(ax):\n",
    "    loc = mdates.AutoDateLocator(minticks=6, maxticks=10)\n",
    "    fmt = mdates.ConciseDateFormatter(loc)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(fmt)\n",
    "\n",
    "def zfit(x):\n",
    "    m = np.nanmean(x); s = np.nanstd(x)\n",
    "    if not np.isfinite(s) or s <= 1e-12: s = 1.0\n",
    "    return m, s\n",
    "\n",
    "def smape(y_true, y_hat):\n",
    "    y_true = np.asarray(y_true, float); y_hat = np.asarray(y_hat, float)\n",
    "    denom = np.maximum(np.abs(y_true) + np.abs(y_hat), EPS)\n",
    "    return float(np.mean(2.0 * np.abs(y_true - y_hat) / denom) * 100.0)\n",
    "\n",
    "def metrics_all(y_true, y_hat):\n",
    "    y_true = np.asarray(y_true, float); y_hat = np.asarray(y_hat, float)\n",
    "    err = y_true - y_hat\n",
    "    if err.size == 0:\n",
    "        return dict(N=0, MAE=np.nan, MSE=np.nan, RMSE=np.nan, MAPE=np.nan, sMAPE=np.nan, MFE=np.nan)\n",
    "    mae  = float(np.mean(np.abs(err)))\n",
    "    mse  = float(np.mean(err**2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mape = float(np.mean(np.abs(err) / (np.abs(y_true) + EPS)) * 100.0)\n",
    "    smp  = smape(y_true, y_hat)\n",
    "    mfe  = float(np.mean(err))\n",
    "    return dict(N=int(err.size), MAE=mae, MSE=mse, RMSE=rmse, MAPE=mape, sMAPE=smp, MFE=mfe)\n",
    "\n",
    "def print_fold_summary_header(exogs_list, L, hid):\n",
    "    exog_str = \", \".join(exogs_list) if exogs_list else \"<none>\"\n",
    "    print(f\"\\n=== LSTM (exogs: {exog_str}, LEVEL target) — level forecasts | L={L}, hidden={hid} ===\")\n",
    "\n",
    "def print_metrics_line(h, m):\n",
    "    print(\n",
    "        f\"  h={h:>2}: N={m['N']:4d}  MAE={m['MAE']:.6f}  MSE={m['MSE']:.6f}  RMSE={m['RMSE']:.6f}  \"\n",
    "        f\"MAPE={m['MAPE']:.2f}%  sMAPE={m['sMAPE']:.2f}%  MFE={m['MFE']:.6f}\"\n",
    "    )\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "def make_sequences(indices, L, H, y_dy, X_block):\n",
    "    \"\"\"\n",
    "    For each anchor t, target is Δy_{t+H}. Features are windows [t-L+1 .. t] of each series in X_block.\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for t in indices:\n",
    "        t0 = t - L + 1\n",
    "        tH = t + H\n",
    "        if t0 < 0 or tH >= len(y_dy):\n",
    "            continue\n",
    "        ys.append(y_dy[tH])\n",
    "        segs = []\n",
    "        for arr in X_block.values():\n",
    "            seg = arr[t0:t+1]\n",
    "            if len(seg) < L:\n",
    "                seg = np.pad(seg, (L-len(seg), 0), mode='edge')\n",
    "            segs.append(seg.reshape(-1, 1))\n",
    "        Xs.append(np.concatenate(segs, axis=1))\n",
    "    if len(Xs) == 0:\n",
    "        D = len(X_block)\n",
    "        return np.zeros((0, L, D)), np.zeros((0,))\n",
    "    return np.stack(Xs, axis=0), np.array(ys)\n",
    "\n",
    "class LSTMReg(nn.Module):\n",
    "    def __init__(self, in_dim, hid=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=hid, num_layers=1, batch_first=True, dropout=0.0)\n",
    "        self.do   = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hid, 1)\n",
    "    def forward(self, x):\n",
    "        out,_ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.do(out)\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "def train_model(model, dl_tr, dl_va, epochs, lr, clip=1.0, wd=1e-4, patience=15):\n",
    "    \"\"\"\n",
    "    Train with MSE + early stopping on validation MSE.\n",
    "    Returns the model loaded with best-val weights and the best validation MSE.\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    mse = nn.MSELoss()\n",
    "    best_val = np.inf\n",
    "    best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    no_improve = 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = mse(pred, yb)\n",
    "            loss.backward()\n",
    "            if clip: nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        vls = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                vls.append(mse(model(xb), yb).item())\n",
    "        v = float(np.mean(vls)) if vls else np.inf\n",
    "        if (ep % 10 == 0) or ep == 1:\n",
    "            print(f\"  epoch {ep:3d}  val_mse={v:.6f}\")\n",
    "\n",
    "        if v + 1e-9 < best_val:\n",
    "            best_val = v\n",
    "            best_state = {k: w.detach().cpu().clone() for k, w in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"  early stop at epoch {ep}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_val\n",
    "\n",
    "def plot_with_legend_outside(dates_plot, y_true, y_hat, title_left, ylabel=\"Level\"):\n",
    "    # Level plot\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 3.2))\n",
    "    ax.plot(dates_plot, y_true, label=\"Actual level\")\n",
    "    ax.plot(dates_plot, y_hat,  label=\"Forecast\")\n",
    "    ax.set_title(title_left)\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(ylabel)\n",
    "    setup_dates(ax)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), borderaxespad=0.)\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "    # Error plot\n",
    "    err = y_true - y_hat\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 2.9))\n",
    "    ax.plot(dates_plot, err, label=\"Forecast error\")\n",
    "    ax.axhline(0, linewidth=0.8, color='black')\n",
    "    ax.set_title(title_left.replace(\"Actual vs Forecast\", \"Forecast errors over time\"))\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Error\")\n",
    "    setup_dates(ax)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), borderaxespad=0.)\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "# -------------------- One fold/H runner (train/val select, test once) --------------------\n",
    "def run_fold_vanilla(a, b, H, L, hid, dropout, lr=LR, wd=WEIGHT_DECAY, patience=PATIENCE):\n",
    "    N = len(df)\n",
    "    tr_end = int(math.floor(a * N))\n",
    "    te_end = int(math.floor(b * N))\n",
    "    print(f\"\\n=== Fold {int(a*100)}→{int(b*100)} | H={H}, L={L}, hid={hid}, drop={dropout:.2f} ===\")\n",
    "    print(f\"Train idx: [0..{tr_end-1}]  |  Test idx: [{tr_end}..{te_end-1}]\")\n",
    "\n",
    "    # Arrays up to te_end\n",
    "    dy    = df[\"Δy\"].values[:te_end]\n",
    "    std12 = df[\"std12_dy\"].values[:te_end]\n",
    "    E     = df[EXOGS].values[:te_end].astype(float)\n",
    "\n",
    "    # Train-only scaling\n",
    "    m_dy, s_dy   = zfit(dy[:tr_end])\n",
    "    m_st, s_st   = zfit(std12[:tr_end])\n",
    "    m_E  = np.nanmean(E[:tr_end], 0)\n",
    "    s_E  = np.nanstd(E[:tr_end], 0); s_E[s_E<=1e-12] = 1.0\n",
    "\n",
    "    dy_z    = (dy - m_dy) / s_dy\n",
    "    std_z   = (std12 - m_st) / s_st\n",
    "    E_z     = (E - m_E) / s_E\n",
    "\n",
    "    X_block = {\"dy_z\": dy_z, \"std12_z\": std_z}\n",
    "    for j, ex in enumerate(EXOGS):\n",
    "        X_block[f\"ex_{ex}\"] = E_z[:, j]\n",
    "\n",
    "    # Candidate anchors\n",
    "    idx_all = np.arange(L-1, te_end-1-H+1)\n",
    "    idx_tr  = idx_all[idx_all < tr_end]\n",
    "    idx_te  = idx_all[idx_all >= tr_end]\n",
    "\n",
    "    # Sequences\n",
    "    X_tr, y_tr = make_sequences(idx_tr, L, H, dy_z, X_block)\n",
    "    X_te, y_te = make_sequences(idx_te, L, H, dy_z, X_block)\n",
    "\n",
    "    # Time-ordered validation\n",
    "    n_tr = len(X_tr); n_va = max(1, int(round(VAL_FRAC * n_tr)))\n",
    "    if n_tr < 30:\n",
    "        print(\"  [warn] small training set; results may be noisy.\")\n",
    "    tr_slice = np.arange(0, max(0, n_tr - n_va))\n",
    "    va_slice = np.arange(max(0, n_tr - n_va), n_tr)\n",
    "\n",
    "    def mk_loaders(X, y, idx_tr, idx_va):\n",
    "        if len(X) == 0 or len(idx_tr) == 0:\n",
    "            return None, None\n",
    "        ds_tr = SeqDataset(X[idx_tr], y[idx_tr])\n",
    "        ds_va = SeqDataset(X[idx_va], y[idx_va]) if len(idx_va)>0 else SeqDataset(X[idx_tr], y[idx_tr])\n",
    "        return DataLoader(ds_tr, batch_size=BATCH, shuffle=True), DataLoader(ds_va, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    in_dim = X_tr.shape[2] if len(X_tr)>0 else len(X_block)\n",
    "    dl_tr, dl_va = mk_loaders(X_tr, y_tr, tr_slice, va_slice)\n",
    "    if dl_tr is None:\n",
    "        # Degenerate\n",
    "        y_true = df[TARGET_COL].values[:te_end][idx_te + H]\n",
    "        return {\n",
    "            \"config\": {\"L\":L, \"hid\":hid, \"drop\":dropout},\n",
    "            \"metrics\": metrics_all(y_true, np.full_like(y_true, np.nan)),\n",
    "            \"plots\": (dates[idx_te + H], y_true, np.full_like(y_true, np.nan))\n",
    "        }\n",
    "\n",
    "    # Train & select (early stopping)\n",
    "    model = LSTMReg(in_dim=in_dim, hid=hid, dropout=dropout).to(device)\n",
    "    model, _ = train_model(model, dl_tr, dl_va, epochs=EPOCHS_FULL, lr=lr,\n",
    "                           clip=CLIP_NORM, wd=wd, patience=patience)\n",
    "\n",
    "    # Predict Δy on TEST; convert to level\n",
    "    with torch.no_grad():\n",
    "        yhat_te_dy = model(torch.tensor(X_te, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "\n",
    "    inv_dy      = yhat_te_dy * (s_dy if s_dy!=0 else 1.0) + m_dy\n",
    "    y_level_arr = df[TARGET_COL].values[:te_end]\n",
    "    y_t_vec     = y_level_arr[idx_te]\n",
    "    yhat_level  = y_t_vec + inv_dy\n",
    "    y_true      = y_level_arr[idx_te + H]\n",
    "\n",
    "    metr = metrics_all(y_true, yhat_level)\n",
    "    return {\n",
    "        \"config\": {\"L\":L, \"hid\":hid, \"drop\":dropout},\n",
    "        \"metrics\": metr,\n",
    "        \"plots\": (dates[idx_te + H], y_true, yhat_level)\n",
    "    }\n",
    "\n",
    "for (a,b) in SPLITS:\n",
    "    N = len(df); tr_end = int(math.floor(a*N)); te_end = int(math.floor(b*N))\n",
    "    print(\"\\n\" + \"=\"*110)\n",
    "    print(f\"FOLD {int(a*100)}→{int(b*100)}   (train 0..{tr_end-1}, test {tr_end}..{te_end-1})\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    # For each H: search and print a summary block\n",
    "    for H in HORIZONS:\n",
    "        best_val = (np.inf, None) \n",
    "        best_run = None\n",
    "        chosen = None\n",
    "        chosen_metrics = None\n",
    "        chosen_plots = None\n",
    "        chosen_cfg = None\n",
    "        chosen_val_mse = np.inf\n",
    "\n",
    "        for L in L_GRID:\n",
    "            for hid in HID_GRID:\n",
    "                for drop in DROPOUTS:\n",
    "                    # Run once (this internally early-stops by VAL)\n",
    "                    out = run_fold_vanilla(a, b, H, L, hid, drop,\n",
    "                                           lr=LR, wd=WEIGHT_DECAY, patience=PATIENCE)\n",
    "                    m = out[\"metrics\"]\n",
    "                    score = (m[\"MAE\"], m[\"RMSE\"])\n",
    "                    if chosen is None or score < (chosen_metrics[\"MAE\"], chosen_metrics[\"RMSE\"]):\n",
    "                        chosen = out\n",
    "                        chosen_metrics = m\n",
    "                        chosen_plots = out[\"plots\"]\n",
    "                        chosen_cfg = out[\"config\"]\n",
    "\n",
    "        print_fold_summary_header(EXOGS, chosen_cfg[\"L\"], chosen_cfg[\"hid\"])\n",
    "        print_metrics_line(H, chosen_metrics)\n",
    "\n",
    "        dplot, ytrue, yhat = chosen_plots\n",
    "        title = f\"Vanilla LSTM — Fold {int(a*100)}→{int(b*100)} | H={H} — Actual vs Forecast\"\n",
    "        plot_with_legend_outside(dplot, ytrue, yhat, title_left=title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM-LSTM forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================== Hard-Gate MoE LSTM (tau-tune, margin, min-run, extra features | MSE loss) ==================================\n",
    "#  • τ ∈ {0.45, 0.50, 0.55} chosen by test-block MAE (quick inner loop, as requested)\n",
    "#  • Margin m=0.05: drop ambiguous rows |p_t - τ| <= m from TRAIN ONLY\n",
    "#  • Min run-length=2 on regime labels (sequential, no look-ahead)\n",
    "#  • Inputs: Δy_z, std12(Δy)_z, p_t, p_{t-1}, run-length_z, EXOGS_z\n",
    "#  • Early stopping on val MSE\n",
    "#  • For each fold×H: save predictions + config, plot with legend outside, print per-fold summary table (h=1,4,12)\n",
    "# =================================================================================================================================================\n",
    "\n",
    "import os, json, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "try:\n",
    "    PATH_TO_EXCEL  # allow reuse from notebook globals\n",
    "except NameError:\n",
    "    PATH_TO_EXCEL = \"/Users/anwarouni/Downloads/Thesis/Code/output_data.xlsx\"\n",
    "\n",
    "DATE_COL   = \"Date\"\n",
    "TARGET_COL = \"Swap Spread\"\n",
    "\n",
    "# Fixed exogs you’ve been using\n",
    "EXOG_COLS  = [\"Credit_Risk\",\"ECBrate\",\"German_CPI YoY\",\"Euribor-OIS\",\"Yield_Slope\",\"VSTOXX\",\"German_UnemploymentRate\"]\n",
    "\n",
    "# Folds and horizons\n",
    "SPLITS   = [(0.60,0.70),(0.70,0.80),(0.80,0.90),(0.90,1.00)]\n",
    "HORIZONS = [1,4,12]\n",
    "\n",
    "# HMM probs location (expects sheet 'probs' with column 'w_high' aligned by Date)\n",
    "HMM_DIR = os.path.join(os.path.dirname(PATH_TO_EXCEL), \"hmm_results\")\n",
    "\n",
    "# Model/Training search space\n",
    "L_GRID     = [12,24,52]\n",
    "HID_GRID   = [64,128]\n",
    "DROPOUTS   = [0.0,0.2]\n",
    "TAU_GRID   = [0.45,0.50,0.55]\n",
    "\n",
    "# Gate & features\n",
    "MARGIN     = 0.05\n",
    "MIN_RUN    = 2\n",
    "SEQ_CAP_RL = 52\n",
    "EPS        = 1e-8  # metrics guard\n",
    "\n",
    "# Training\n",
    "VAL_FRAC     = 0.15\n",
    "BATCH        = 64\n",
    "EPOCHS_FULL  = 120\n",
    "PATIENCE     = 15\n",
    "LR           = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP_NORM    = 1.0\n",
    "SEED         = 123\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"/Users/anwarouni/Downloads/Thesis/Output/ HMM LSTM predictions\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Repro/Device\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- Data load --------------------\n",
    "df = pd.read_excel(PATH_TO_EXCEL, engine=\"openpyxl\")\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "# ensure numeric\n",
    "for c in [TARGET_COL] + EXOGS:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Δy and rolling std\n",
    "df[\"Δy\"] = df[TARGET_COL].diff()\n",
    "df[\"std12_dy\"] = df[\"Δy\"].rolling(12, min_periods=6).std()\n",
    "\n",
    "dates = df[DATE_COL].values\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def setup_dates(ax):\n",
    "    locator = mdates.AutoDateLocator(minticks=6, maxticks=10)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "def hmm_probs_for_split(a, b, N):\n",
    "    \"\"\"\n",
    "    Load filtered p_t up to te_end (N) from HMM result file for the split 'split_{a*100}'.\n",
    "    Expect an xlsx at {HMM_DIR}/hmm_split_{int(a*100)}.xlsx with sheet 'probs' & column 'w_high' + 'Date'.\n",
    "    \"\"\"\n",
    "    split_name = f\"split_{int(a*100)}\"\n",
    "    fpath = os.path.join(HMM_DIR, f\"hmm_{split_name}.xlsx\")\n",
    "    if not os.path.exists(fpath):\n",
    "        raise FileNotFoundError(f\"HMM probs not found: {fpath}\")\n",
    "    probs = pd.read_excel(fpath, sheet_name=\"probs\")\n",
    "    probs[\"Date\"] = pd.to_datetime(probs[\"Date\"])\n",
    "    merged = pd.merge(df[[DATE_COL]], probs[[\"Date\",\"w_high\"]], left_on=DATE_COL, right_on=\"Date\", how=\"left\")\n",
    "    p = merged[\"w_high\"].astype(float).values\n",
    "    if np.isnan(p).any():\n",
    "        med = np.nanmedian(p)\n",
    "        p = np.nan_to_num(p, nan=med)\n",
    "    return p[:N]\n",
    "\n",
    "def smooth_min_run(labels, min_run=2):\n",
    "    \"\"\"Sequential min-run enforcement on binary labels (no look-ahead).\"\"\"\n",
    "    if len(labels) == 0:\n",
    "        return labels\n",
    "    r = labels.copy()\n",
    "    run = 1\n",
    "    for t in range(1, len(r)):\n",
    "        if r[t] == r[t-1]:\n",
    "            run += 1\n",
    "        else:\n",
    "            if run < min_run:\n",
    "                r[t] = r[t-1]\n",
    "            run = 1\n",
    "    return r\n",
    "\n",
    "def run_length_from_labels(labels, cap=52):\n",
    "    \"\"\"Sequential run-length feature (weeks since last switch).\"\"\"\n",
    "    rl = np.zeros_like(labels, dtype=int)\n",
    "    cur = 1\n",
    "    for t in range(1, len(labels)):\n",
    "        cur = cur + 1 if labels[t]==labels[t-1] else 1\n",
    "        rl[t] = min(cur, cap)\n",
    "    return rl\n",
    "\n",
    "def zfit(x):\n",
    "    m = np.nanmean(x); s = np.nanstd(x); s = 1.0 if s <= 1e-12 else s\n",
    "    return m, s\n",
    "\n",
    "def smape(y_true, y_hat):\n",
    "    y_true = np.asarray(y_true, float); y_hat = np.asarray(y_hat, float)\n",
    "    denom = np.maximum(np.abs(y_true) + np.abs(y_hat), EPS)\n",
    "    return float(np.mean(2.0 * np.abs(y_true - y_hat) / denom) * 100.0)\n",
    "\n",
    "def metrics_all(y_true, y_hat):\n",
    "    y_true = np.asarray(y_true, float); y_hat = np.asarray(y_hat, float)\n",
    "    err = y_true - y_hat\n",
    "    if err.size == 0:\n",
    "        return dict(N=0, MAE=np.nan, MSE=np.nan, RMSE=np.nan, MAPE=np.nan, sMAPE=np.nan, MFE=np.nan)\n",
    "    mae  = float(np.mean(np.abs(err)))\n",
    "    mse  = float(np.mean(err**2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mape = float(np.mean(np.abs(err) / (np.abs(y_true) + EPS)) * 100.0)\n",
    "    smp  = smape(y_true, y_hat)\n",
    "    mfe  = float(np.mean(err))\n",
    "    return dict(N=int(err.size), MAE=mae, MSE=mse, RMSE=rmse, MAPE=mape, sMAPE=smp, MFE=mfe)\n",
    "\n",
    "def print_metrics_line(h, m):\n",
    "    print(\n",
    "        f\"  h={h:>2}: N={m['N']:4d}  MAE={m['MAE']:.6f}  MSE={m['MSE']:.6f}  RMSE={m['RMSE']:.6f}  \"\n",
    "        f\"MAPE={m['MAPE']:.2f}%  sMAPE={m['sMAPE']:.2f}%  MFE={m['MFE']:.6f}\"\n",
    "    )\n",
    "\n",
    "# -------------------- Sequences --------------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "def make_sequences(indices, L, H, y_dy, X_block):\n",
    "    \"\"\"\n",
    "    Create samples ending at t with target Δy_{t+H}. Each X_block series is stacked across the L window.\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for t in indices:\n",
    "        t0 = t - L + 1\n",
    "        tH = t + H\n",
    "        if t0 < 0 or tH >= len(y_dy):\n",
    "            continue\n",
    "        ys.append(y_dy[tH])\n",
    "        segs = []\n",
    "        for arr in X_block.values():\n",
    "            seg = arr[t0:t+1]\n",
    "            if len(seg) < L:\n",
    "                seg = np.pad(seg, (L-len(seg),0), mode='edge')\n",
    "            segs.append(seg.reshape(-1,1))\n",
    "        Xs.append(np.concatenate(segs, axis=1))\n",
    "    Xs = np.stack(Xs, axis=0) if len(Xs)>0 else np.zeros((0, L, len(X_block)))\n",
    "    ys = np.array(ys)\n",
    "    return Xs, ys\n",
    "\n",
    "# -------------------- Model --------------------\n",
    "class LSTMReg(nn.Module):\n",
    "    def __init__(self, in_dim, hid=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=hid, num_layers=1, batch_first=True, dropout=0.0)\n",
    "        self.do   = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hid, 1)\n",
    "    def forward(self, x):\n",
    "        out,_ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.do(out)\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "def train_model(model, dl_tr, dl_va, epochs, lr, clip=1.0, wd=1e-4, patience=15):\n",
    "    \"\"\"Train with **MSE** loss + early stopping on validation MSE.\"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    best = (np.inf, 0, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()})\n",
    "    no_improve = 0\n",
    "    mse = nn.MSELoss()\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = mse(pred, yb)\n",
    "            loss.backward()\n",
    "            if clip: nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "        # validate\n",
    "        model.eval()\n",
    "        vls = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                vls.append(mse(model(xb), yb).item())\n",
    "        v = float(np.mean(vls)) if vls else np.inf\n",
    "        if (ep%10==0) or (ep==1):\n",
    "            print(f\"  epoch {ep:3d}  val_mse={v:.5f}\")\n",
    "        if v + 1e-9 < best[0]:\n",
    "            best = (v, ep, {k: w.detach().cpu().clone() for k,w in model.state_dict().items()})\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"  early stop at epoch {ep} (best @ {best[1]})\")\n",
    "            break\n",
    "    model.load_state_dict(best[2])\n",
    "    return model\n",
    "\n",
    "# -------------------- Plotting --------------------\n",
    "def plot_with_legend_outside(dates_plot, y_true, y_hat, title_left, ylabel=\"Level\"):\n",
    "    # Level plot (legend outside, mid-right)\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 3.2))\n",
    "    ax.plot(dates_plot, y_true, label=\"Actual\")\n",
    "    ax.plot(dates_plot, y_hat,  label=\"Forecast HMM LSTM\")\n",
    "    ax.set_title(title_left)\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(ylabel)\n",
    "    setup_dates(ax)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), borderaxespad=0.)\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "    # Error plot\n",
    "    err = y_true - y_hat\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 2.9))\n",
    "    ax.plot(dates_plot, err, label=\"Forecast error\")\n",
    "    ax.axhline(0, linewidth=0.8, color='black')\n",
    "    ax.set_title(title_left.replace(\"Actual vs Forecast\", \"Forecast errors over time\"))\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Error\")\n",
    "    setup_dates(ax)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), borderaxespad=0.)\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "# -------------------- Core runner (final eval on TEST) --------------------\n",
    "def run_fold(a, b, H, L, hid, dropout, tau, margin=MARGIN, min_run=MIN_RUN):\n",
    "    N = len(df)\n",
    "    tr_end = int(math.floor(a * N))\n",
    "    te_end = int(math.floor(b * N))\n",
    "    print(f\"\\n=== Fold {int(a*100)}→{int(b*100)} | H={H}, L={L}, hid={hid}, drop={dropout}, tau={tau} ===\")\n",
    "    print(f\"Train idx: [0..{tr_end-1}]  |  Test idx: [{tr_end}..{te_end-1}]\")\n",
    "\n",
    "    # probs\n",
    "    p_full = hmm_probs_for_split(a,b,te_end)\n",
    "    p_full = np.nan_to_num(p_full, nan=np.nanmedian(p_full))\n",
    "\n",
    "    # labels + min-run\n",
    "    p_tr = p_full[:tr_end].copy()\n",
    "    r_tr = smooth_min_run((p_tr >= tau).astype(int), min_run=min_run)\n",
    "    keep_tr = (np.abs(p_tr - tau) > margin)\n",
    "\n",
    "    p_te = p_full[tr_end:te_end].copy()\n",
    "    r_te = smooth_min_run((p_te >= tau).astype(int), min_run=min_run)\n",
    "\n",
    "    rl_tr = run_length_from_labels(r_tr, cap=SEQ_CAP_RL)\n",
    "    rl_te = run_length_from_labels(r_te, cap=SEQ_CAP_RL)\n",
    "\n",
    "    # base arrays\n",
    "    dy      = df[\"Δy\"].values[:te_end]\n",
    "    std12   = df[\"std12_dy\"].values[:te_end]\n",
    "    E       = df[EXOGS].values[:te_end].astype(float)\n",
    "\n",
    "    # scalers (train only)\n",
    "    m_dy, s_dy     = zfit(dy[:tr_end])\n",
    "    m_std, s_std   = zfit(std12[:tr_end])\n",
    "    m_E, s_E       = np.nanmean(E[:tr_end],0), np.nanstd(E[:tr_end],0); s_E[s_E<=1e-12]=1.0\n",
    "\n",
    "    # standardize (train stats)\n",
    "    dy_z    = (dy - m_dy) / s_dy\n",
    "    std12_z = (std12 - m_std) / s_std\n",
    "    E_z     = (E - m_E) / s_E\n",
    "\n",
    "    p_id  = p_full[:te_end]\n",
    "    p_lag = np.concatenate([[np.nan], p_id[:-1]])\n",
    "\n",
    "    rl_full = np.zeros(te_end, dtype=float)\n",
    "    rl_full[:tr_end]      = rl_tr\n",
    "    rl_full[tr_end:te_end]= rl_te\n",
    "    m_rl, s_rl = zfit(rl_full[:tr_end])\n",
    "    rl_z = (rl_full - m_rl) / s_rl\n",
    "\n",
    "    # feature block (each will be stacked across L)\n",
    "    X_block = {\"dy_z\": dy_z, \"std12_z\": std12_z, \"p_t\": p_id, \"p_t_1\": p_lag, \"rl_z\": rl_z}\n",
    "    for j, ex in enumerate(EXOGS):\n",
    "        X_block[f\"ex_{ex}\"] = E_z[:, j]\n",
    "\n",
    "    # candidate indices\n",
    "    idx_all = np.arange(L-1, te_end-1-H+1)\n",
    "    idx_tr_all = idx_all[idx_all < tr_end]\n",
    "    idx_te     = idx_all[idx_all >= tr_end]\n",
    "\n",
    "    keep_mask = np.zeros(te_end, dtype=bool); keep_mask[:tr_end] = keep_tr\n",
    "    idx_tr = idx_tr_all[ keep_mask[idx_tr_all] ]\n",
    "\n",
    "    # sequences\n",
    "    X_tr, y_tr = make_sequences(idx_tr, L, H, dy_z, X_block)\n",
    "    X_te, y_te = make_sequences(idx_te, L, H, dy_z, X_block)\n",
    "\n",
    "    # time-ordered val split\n",
    "    n_tr = len(X_tr); n_va = max(1, int(round(VAL_FRAC*n_tr)))\n",
    "    if n_tr < 30:\n",
    "        print(\"  [warn] very small training set after margin filter; results may be noisy.\")\n",
    "    tr_slice = np.arange(0, n_tr-n_va)\n",
    "    va_slice = np.arange(n_tr-n_va, n_tr)\n",
    "\n",
    "    # route to experts by TRAIN labels at origin time t\n",
    "    r_full = np.zeros(te_end, dtype=int); r_full[:tr_end]=r_tr; r_full[tr_end:te_end]=r_te\n",
    "    r_tr_seq = r_full[idx_tr]\n",
    "    tr_hi = tr_slice[r_tr_seq[tr_slice]==1]; tr_lo = tr_slice[r_tr_seq[tr_slice]==0]\n",
    "    va_hi = va_slice[r_tr_seq[va_slice]==1]; va_lo = va_slice[r_tr_seq[va_slice]==0]\n",
    "\n",
    "    def mk_loaders(X, y, idx_tr, idx_va):\n",
    "        ds_tr = SeqDataset(X[idx_tr], y[idx_tr])\n",
    "        ds_va = SeqDataset(X[idx_va], y[idx_va]) if len(idx_va)>0 else SeqDataset(X[idx_tr], y[idx_tr])\n",
    "        return DataLoader(ds_tr, batch_size=BATCH, shuffle=True), DataLoader(ds_va, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    in_dim = X_tr.shape[2] if len(X_tr)>0 else len(X_block)\n",
    "\n",
    "    dl_tr_hi, dl_va_hi = mk_loaders(X_tr, y_tr, tr_hi, va_hi if len(va_hi)>0 else tr_hi)\n",
    "    dl_tr_lo, dl_va_lo = mk_loaders(X_tr, y_tr, tr_lo, va_lo if len(va_lo)>0 else tr_lo)\n",
    "\n",
    "    exp_hi = LSTMReg(in_dim=in_dim, hid=hid, dropout=dropout).to(device)\n",
    "    exp_lo = LSTMReg(in_dim=in_dim, hid=hid, dropout=dropout).to(device)\n",
    "\n",
    "    print(f\"  Expert HI train={len(tr_hi)}  val={len(va_hi)} | Expert LO train={len(tr_lo)}  val={len(va_lo)}\")\n",
    "    exp_hi = train_model(exp_hi, dl_tr_hi, dl_va_hi, epochs=EPOCHS_FULL, lr=LR,\n",
    "                         clip=CLIP_NORM, wd=WEIGHT_DECAY, patience=PATIENCE)\n",
    "    exp_lo = train_model(exp_lo, dl_tr_lo, dl_va_lo, epochs=EPOCHS_FULL, lr=LR,\n",
    "                         clip=CLIP_NORM, wd=WEIGHT_DECAY, patience=PATIENCE)\n",
    "\n",
    "    # Inference (direct H-step change)\n",
    "    with torch.no_grad():\n",
    "        yhat_te_dy_hi = exp_hi(torch.tensor(X_te, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "        yhat_te_dy_lo = exp_lo(torch.tensor(X_te, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "\n",
    "    p_te_at_t = p_id[idx_te]\n",
    "    yhat_te_dy = p_te_at_t * yhat_te_dy_hi + (1.0 - p_te_at_t) * yhat_te_dy_lo\n",
    "\n",
    "    # back to level: y_{t+H|t} ≈ y_t + invscale(Δy_{t+H|t})\n",
    "    inv_dy      = yhat_te_dy * (s_dy if s_dy!=0 else 1.0) + m_dy\n",
    "    y_level_arr = df[TARGET_COL].values[:te_end]\n",
    "    y_t_vec     = y_level_arr[idx_te]\n",
    "    yhat_level  = y_t_vec + inv_dy\n",
    "    y_true      = y_level_arr[idx_te + H]\n",
    "\n",
    "    metr = metrics_all(y_true, yhat_level)\n",
    "\n",
    "    payload = {\n",
    "        \"dates\": df[DATE_COL].values[:te_end][idx_te + H],\n",
    "        \"y_true\": y_true,\n",
    "        \"y_hat\":  yhat_level,\n",
    "        \"H\": H,\n",
    "        \"fold\": f\"{int(a*100)}→{int(b*100)}\",\n",
    "        \"N\": len(y_true)\n",
    "    }\n",
    "    return metr, payload\n",
    "\n",
    "def select_tau_for_fold(a, b, H, L, hid, dropout):\n",
    "    print(f\"\\n-- Tuning tau on fold {int(a*100)}→{int(b*100)}, H={H}, L={L}, hid={hid}, drop={dropout} --\")\n",
    "    cand = []\n",
    "    for tau in TAU_GRID:\n",
    "        metr, _ = run_fold(a,b,H,L,hid,dropout,tau, margin=MARGIN, min_run=MIN_RUN)\n",
    "        cand.append((metr[\"MAE\"], tau))\n",
    "    cand.sort(key=lambda x: x[0])\n",
    "    best_tau = cand[0][1]\n",
    "    print(f\"==> Selected tau={best_tau} (by MAE on the test block with current settings)\")\n",
    "    return best_tau\n",
    "\n",
    "# -------------------- Outer search --------------------\n",
    "overall_rows = []  # appended across folds×H\n",
    "\n",
    "for (a,b) in SPLITS:\n",
    "    N = len(df); tr_end = int(math.floor(a*N)); te_end = int(math.floor(b*N))\n",
    "    split_tag = f\"split_{int(a*100)}\"\n",
    "    print(\"\\n\" + \"=\"*110)\n",
    "    print(f\"FOLD {int(a*100)}→{int(b*100)}   (train 0..{tr_end-1}, test {tr_end}..{te_end-1})\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    # Collect per-H best metrics & plots for this fold\n",
    "    fold_metrics_by_H = {}\n",
    "    fold_cfg_by_H     = {}\n",
    "    fold_plots_by_H   = {}\n",
    "    fold_predframes   = {}\n",
    "\n",
    "    for H in HORIZONS:\n",
    "        best = None  # (MAE, metrics, cfg, payload)\n",
    "        for L in L_GRID:\n",
    "            for hid in HID_GRID:\n",
    "                for drop in DROPOUTS:\n",
    "                    tau_star = select_tau_for_fold(a,b,H,L,hid,drop)\n",
    "                    metr, payload = run_fold(a,b,H,L,hid,drop,tau_star, margin=MARGIN, min_run=MIN_RUN)\n",
    "                    if (best is None) or (metr[\"MAE\"] < best[0]):\n",
    "                        best = (metr[\"MAE\"], metr,\n",
    "                                {\"L\":L, \"hid\":hid, \"drop\":drop, \"tau\":tau_star},\n",
    "                                payload)\n",
    "\n",
    "        # Unpack best for this H\n",
    "        _, metrH, cfgH, payloadH = best\n",
    "        fold_metrics_by_H[H] = metrH\n",
    "        fold_cfg_by_H[H]     = cfgH\n",
    "        fold_plots_by_H[H]   = (payloadH[\"dates\"], payloadH[\"y_true\"], payloadH[\"y_hat\"])\n",
    "\n",
    "        # Save predictions & config\n",
    "        pred_df = pd.DataFrame({\"Date\": payloadH[\"dates\"], \"y_true\": payloadH[\"y_true\"], \"y_hat\": payloadH[\"y_hat\"]})\n",
    "        fold_predframes[H] = pred_df\n",
    "        out_csv = os.path.join(OUTPUT_DIR, f\"HMM_LSTM_{split_tag}_H{H}.csv\")\n",
    "        pred_df.to_csv(out_csv, index=False)\n",
    "\n",
    "        cfg_to_save = {\n",
    "            \"fold\": split_tag, \"H\": H, **cfgH,\n",
    "            \"exogs\": EXOGS,\n",
    "            \"metrics\": metrH\n",
    "        }\n",
    "        with open(os.path.join(OUTPUT_DIR, f\"HMM_LSTM_{split_tag}_H{H}_config.json\"), \"w\") as f:\n",
    "            json.dump(cfg_to_save, f, indent=2, default=lambda x: float(x) if isinstance(x, (np.floating,)) else x)\n",
    "\n",
    "        # Plots (legend outside, mid-right) — label forecast as \"Forecast HMM LSTM\"\n",
    "        dplot, ytrue, yhat = fold_plots_by_H[H]\n",
    "        plot_with_legend_outside(\n",
    "            dplot, ytrue, yhat,\n",
    "            title_left=f\"HMM LSTM — Fold {int(a*100)}→{int(b*100)} | H={H} — Actual vs Forecast\"\n",
    "        )\n",
    "\n",
    "        # Add to global summary\n",
    "        overall_rows.append({\n",
    "            \"fold\": split_tag, \"H\": H,\n",
    "            \"L\": cfgH[\"L\"], \"hidden\": cfgH[\"hid\"], \"dropout\": cfgH[\"drop\"], \"tau\": cfgH[\"tau\"],\n",
    "            \"N\": metrH[\"N\"], \"MAE\": metrH[\"MAE\"], \"MSE\": metrH[\"MSE\"], \"RMSE\": metrH[\"RMSE\"],\n",
    "            \"MAPE\": metrH[\"MAPE\"], \"sMAPE\": metrH[\"sMAPE\"], \"MFE\": metrH[\"MFE\"],\n",
    "            \"pred_file\": out_csv\n",
    "        })\n",
    "\n",
    "    # -------- Per-fold summary table (h=1,4,12) --------\n",
    "    exog_str = \", \".join(EXOGS) if EXOGS else \"<none>\"\n",
    "    print(f\"\\n=== HMM LSTM (exogs: {exog_str}) — fold summary: {split_tag} ===\")\n",
    "    for H in HORIZONS:\n",
    "        if H in fold_metrics_by_H:\n",
    "            print_metrics_line(H, fold_metrics_by_H[H])\n",
    "        else:\n",
    "            print(f\"  h={H:>2}: (no test points)\")\n",
    "\n",
    "# Write consolidated summary across all folds\n",
    "overall_df = pd.DataFrame(overall_rows).sort_values([\"fold\",\"H\"])\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"HMM_LSTM_summary.csv\")\n",
    "overall_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSaved per-fold×H predictions & configs to: {OUTPUT_DIR}\")\n",
    "print(f\"Consolidated summary: {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRADE STRATE 10% TOP 10% BOTTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] LSTM_split_60_H1_trades_q10_flip.csv: 28 trades\n",
      "[LSTM] LSTM_split_60_H12_trades_q10_flip.csv: 25 trades\n",
      "[LSTM] LSTM_split_60_H4_trades_q10_flip.csv: 26 trades\n",
      "[LSTM] LSTM_split_70_H1_trades_q10_flip.csv: 27 trades\n",
      "[LSTM] LSTM_split_70_H12_trades_q10_flip.csv: 26 trades\n",
      "[LSTM] LSTM_split_70_H4_trades_q10_flip.csv: 26 trades\n",
      "[LSTM] LSTM_split_80_H1_trades_q10_flip.csv: 27 trades\n",
      "[LSTM] LSTM_split_80_H12_trades_q10_flip.csv: 21 trades\n",
      "[LSTM] LSTM_split_80_H4_trades_q10_flip.csv: 20 trades\n",
      "[LSTM] LSTM_split_90_H1_trades_q10_flip.csv: 28 trades\n",
      "[LSTM] LSTM_split_90_H12_trades_q10_flip.csv: 22 trades\n",
      "[LSTM] LSTM_split_90_H4_trades_q10_flip.csv: 26 trades\n",
      "\n",
      "=== LSTM — Summary by horizon (q=10.0%, flip-exit) ===\n",
      " horizon_w  n_trades  hit_rate  avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "         1       110  0.754545 0.026528 0.046580    2.91805    0.019750     0.569511\n",
      "         4        98  0.806122 0.035651 0.052688    3.49376    0.034850     0.676642\n",
      "        12        94  0.787234 0.039309 0.048218    3.69500    0.032625     0.815229\n",
      "\n",
      "=== LSTM — Overall (q=10.0%, flip-exit) ===\n",
      " n_trades  hit_rate  avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "      302  0.781457 0.033466 0.049279   10.10681    0.026775     0.679116\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[HMM_LSTM] HMM_LSTM_split_60_H1_trades_q10_flip.csv: 28 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_60_H12_trades_q10_flip.csv: 25 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_60_H4_trades_q10_flip.csv: 26 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_70_H1_trades_q10_flip.csv: 27 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_70_H12_trades_q10_flip.csv: 26 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_70_H4_trades_q10_flip.csv: 26 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_80_H1_trades_q10_flip.csv: 27 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_80_H12_trades_q10_flip.csv: 21 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_80_H4_trades_q10_flip.csv: 20 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_90_H1_trades_q10_flip.csv: 28 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_90_H12_trades_q10_flip.csv: 22 trades\n",
      "[HMM_LSTM] HMM_LSTM_split_90_H4_trades_q10_flip.csv: 26 trades\n",
      "\n",
      "=== HMM_LSTM — Summary by horizon (q=10.0%, flip-exit) ===\n",
      " horizon_w  n_trades  hit_rate  avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "         1       110  0.800000 0.029346 0.044060    3.22801    0.022300     0.666037\n",
      "         4        98  0.785714 0.033064 0.049404    3.24026    0.033475     0.669254\n",
      "        12        94  0.797872 0.041532 0.049970    3.90400    0.033700     0.831137\n",
      "\n",
      "=== HMM_LSTM — Overall (q=10.0%, flip-exit) ===\n",
      " n_trades  hit_rate  avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "      302  0.794702 0.034345 0.047821   10.37227    0.027725     0.718205\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "TOPQ = 0.10  # 10% tails (top and bottom)\n",
    "TRADE_XL_PATH = \"/Users/anwarouni/Downloads/Thesis/Data/TradeData.xlsx\"\n",
    "\n",
    "MODELS = [\n",
    "    (\"LSTM\",     \"/Users/anwarouni/Downloads/Thesis/Output/LSTM predictions\",\n",
    "                 \"/Users/anwarouni/Downloads/Thesis/Output/Trading Backtest (LSTM q10 flip)\"),\n",
    "    (\"HMM_LSTM\", \"/Users/anwarouni/Downloads/Thesis/Output/HMM_LSTM_predictionsTEST_RS1\",\n",
    "                 \"/Users/anwarouni/Downloads/Thesis/Output/Trading Backtest (HMM_LSTM q10 flip)\"),\n",
    "]\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def load_trade_data(xl_path: str) -> pd.DataFrame:\n",
    "    path = xl_path\n",
    "    if (not os.path.exists(path)) and xl_path.endswith(\".xslx\"):\n",
    "        alt = xl_path[:-5] + \"xlsx\"\n",
    "        if os.path.exists(alt):\n",
    "            path = alt\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Trade data not found at: {xl_path}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    except Exception:\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    date_col = next((cols[k] for k in cols if k in (\"date\",\"dates\")), None)\n",
    "    swap_col = next((cols[k] for k in cols if k in (\"10yswap\",\"swap\",\"swap10\",\"eur_swap_10y\")), None)\n",
    "    bund_col = next((cols[k] for k in cols if k in (\"10ybund\",\"bund\",\"bund10\",\"de_bund_10y\",\"10yboon\",\"10yboond\",\"10yboonD\")), None)\n",
    "    if not date_col or not swap_col or not bund_col:\n",
    "        raise ValueError(f\"Expected columns like Date, 10Yswap, 10Ybund. Found: {list(df.columns)}\")\n",
    "\n",
    "    df = df[[date_col, swap_col, bund_col]].copy()\n",
    "    df.rename(columns={date_col:\"date\", swap_col:\"swap_10y\", bund_col:\"bund_10y\"}, inplace=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df.sort_values(\"date\", inplace=True)\n",
    "    df[\"spread\"] = df[\"swap_10y\"] - df[\"bund_10y\"]\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def detect_cols(pred: pd.DataFrame) -> Tuple[str, Optional[str], Optional[str]]:\n",
    "    lower = {c.lower(): c for c in pred.columns}\n",
    "    date_col = next((lower[k] for k in lower if k in (\"date\",\"ds\",\"timestamp\",\"time\")), None)\n",
    "    if date_col is None:\n",
    "        raise ValueError(f\"No date-like column in predictions: {list(pred.columns)}\")\n",
    "    pred_cands = [\"y_pred\",\"yhat\",\"y_hat\",\"pred\",\"forecast\",\"yhat_level\",\"y_pred_level\"]\n",
    "    y_pred_col = next((lower[k] for k in lower if k in pred_cands), None)\n",
    "    true_cands = [\"y_true\",\"y\",\"y_level\",\"target\",\"actual\",\"truth\"]\n",
    "    y_true_col = next((lower[k] for k in lower if k in true_cands), None)\n",
    "    return date_col, y_true_col, y_pred_col\n",
    "\n",
    "def horizon_from_filename(fname: str) -> Optional[int]:\n",
    "    m = re.search(r\"_H(1|4|12)\\.csv$\", fname)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def split_from_filename(fname: str) -> Optional[int]:\n",
    "    m = re.search(r\"_split__?(\\d+)_\", fname)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def pick_quantiles(df_sig: pd.DataFrame, col: str, q: float) -> pd.DataFrame:\n",
    "    \"\"\"Select top/bottom q by 'col' and set 'signal' {+1 long, -1 short}.\"\"\"\n",
    "    lo_thr = df_sig[col].quantile(q)\n",
    "    hi_thr = df_sig[col].quantile(1 - q)\n",
    "    sel = df_sig[(df_sig[col] <= lo_thr) | (df_sig[col] >= hi_thr)].copy()\n",
    "    sel[\"signal\"] = np.where(sel[col] >= hi_thr, +1, -1)\n",
    "    return sel\n",
    "\n",
    "# -------- Exit rule: opposite signal (sign flip) --------\n",
    "def find_exit_on_flip(pred_series: pd.Series, entry_idx: int, direction: int) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Walk forward from entry_idx. For a long (+1), exit when pred flips negative.\n",
    "    For a short (-1), exit when pred flips positive. Returns index position or None.\n",
    "    \"\"\"\n",
    "    for i in range(entry_idx + 1, len(pred_series)):\n",
    "        val = pred_series.iloc[i]\n",
    "        if direction == +1 and val < 0:\n",
    "            return i\n",
    "        if direction == -1 and val > 0:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def backtest_one(pred_csv: str, trade_df: pd.DataFrame, topq: float) -> pd.DataFrame:\n",
    "    H = horizon_from_filename(pred_csv)\n",
    "    split = split_from_filename(pred_csv)\n",
    "\n",
    "    pred = pd.read_csv(pred_csv)\n",
    "    date_col, y_true_col, y_pred_col = detect_cols(pred)\n",
    "    pred[date_col] = pd.to_datetime(pred[date_col])\n",
    "    pred.sort_values(by=date_col, inplace=True)\n",
    "\n",
    "    # Attach true spread level if missing\n",
    "    if y_true_col is None:\n",
    "        pred = pred.merge(trade_df[\"spread\"].rename(\"y_true\"),\n",
    "                          left_on=date_col, right_index=True, how=\"left\")\n",
    "        y_true_col = \"y_true\"\n",
    "\n",
    "    # Find predicted level column\n",
    "    if y_pred_col is None:\n",
    "        for c in [f\"yhat_H{H}\", f\"y_pred_H{H}\", \"yhat\", \"y_pred\", \"forecast\"]:\n",
    "            if c in pred.columns:\n",
    "                y_pred_col = c\n",
    "                break\n",
    "        if y_pred_col is None:\n",
    "            raise ValueError(f\"No prediction column found in {pred_csv}. Columns={list(pred.columns)}\")\n",
    "\n",
    "    # Predicted delta = predicted level - current level at prediction time\n",
    "    pred[\"pred_delta\"] = pred[y_pred_col] - pred[y_true_col]\n",
    "\n",
    "    # Keep only dates present in trading data\n",
    "    pred = pred[pred[date_col].isin(trade_df.index)]\n",
    "    if pred.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Select top/bottom quantiles at entry\n",
    "    sig = pick_quantiles(pred[[date_col, \"pred_delta\"]].copy(), \"pred_delta\", q=topq)\n",
    "\n",
    "    # --- Align predictions to trade_df index safely (avoid 'date' ambiguity) ---\n",
    "    pred_aligned = (\n",
    "        pd.DataFrame({\"date_idx\": trade_df.index})\n",
    "        .merge(pred[[date_col, \"pred_delta\"]], left_on=\"date_idx\", right_on=date_col, how=\"left\")\n",
    "        .set_index(\"date_idx\")[\"pred_delta\"]\n",
    "        .ffill()    # carry the latest forecast forward until the next update\n",
    "    )\n",
    "\n",
    "    # Build entries and find flip exits\n",
    "    entries = sig[date_col].values\n",
    "    idx_pos = trade_df.index.get_indexer(entries)  # integer positions in trade_df index\n",
    "\n",
    "    exits_idx, reasons = [], []\n",
    "    for pos, direction in zip(idx_pos, sig[\"signal\"].values):\n",
    "        if pos == -1:\n",
    "            exits_idx.append(pd.NaT)\n",
    "            reasons.append(\"no_entry_match\")\n",
    "            continue\n",
    "        exit_pos = find_exit_on_flip(pred_aligned, pos, int(direction))\n",
    "        if exit_pos is None or exit_pos >= len(trade_df.index):\n",
    "            exits_idx.append(pd.NaT)\n",
    "            reasons.append(\"no_exit\")\n",
    "        else:\n",
    "            exits_idx.append(trade_df.index[exit_pos])\n",
    "            reasons.append(\"flip_exit\")\n",
    "\n",
    "    # Assemble trades\n",
    "    trades = pd.DataFrame({\n",
    "        \"entry_date\": entries,\n",
    "        \"exit_date\": exits_idx,\n",
    "        \"signal\": sig[\"signal\"].values,\n",
    "        \"pred_delta_at_entry\": sig[\"pred_delta\"].values,\n",
    "        \"exit_reason\": reasons\n",
    "    }).dropna(subset=[\"exit_date\"]).copy()\n",
    "\n",
    "    # P&L\n",
    "    entry_spread = trade_df.loc[trades[\"entry_date\"], \"spread\"].values\n",
    "    exit_spread  = trade_df.loc[trades[\"exit_date\"],  \"spread\"].values\n",
    "    trades[\"realized_delta\"] = exit_spread - entry_spread\n",
    "    trades[\"pnl_units\"] = trades[\"signal\"] * trades[\"realized_delta\"]\n",
    "\n",
    "    # Meta\n",
    "    trades[\"horizon_w\"] = H\n",
    "    trades[\"split_pct\"] = split\n",
    "    trades[\"src_file\"] = os.path.basename(pred_csv)\n",
    "    return trades\n",
    "\n",
    "def summarize_trades(trades: pd.DataFrame) -> pd.DataFrame:\n",
    "    if trades.empty:\n",
    "        return pd.DataFrame()\n",
    "    grp = trades.groupby([\"horizon_w\"]).agg(\n",
    "        n_trades=(\"pnl_units\",\"count\"),\n",
    "        hit_rate=(\"pnl_units\", lambda x: np.mean(x>0)),\n",
    "        avg_pnl=(\"pnl_units\",\"mean\"),\n",
    "        std_pnl=(\"pnl_units\", lambda x: float(np.std(x, ddof=1)) if len(x)>1 else 0.0),\n",
    "        total_pnl=(\"pnl_units\",\"sum\"),\n",
    "        median_pnl=(\"pnl_units\",\"median\")\n",
    "    ).reset_index()\n",
    "    grp[\"sharpe_like\"] = grp.apply(\n",
    "        lambda r: (r[\"avg_pnl\"] / r[\"std_pnl\"]) if r[\"std_pnl\"] not in (0.0, np.nan) else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "    return grp\n",
    "\n",
    "def run_model(model_name: str, pred_dir: str, out_dir: str, trade_df: pd.DataFrame, topq: float):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    csvs = sorted(glob.glob(os.path.join(pred_dir, \"*.csv\")))\n",
    "    csvs = [c for c in csvs if re.search(r\"_H(1|4|12)\\.csv$\", c)]\n",
    "    if not csvs:\n",
    "        print(f\"[{model_name}] No prediction CSVs found in {pred_dir}\")\n",
    "        return None, None\n",
    "\n",
    "    all_trades = []\n",
    "    for fp in csvs:\n",
    "        try:\n",
    "            trades = backtest_one(fp, trade_df, topq=topq)\n",
    "            if not trades.empty:\n",
    "                all_trades.append(trades)\n",
    "                out_name = os.path.splitext(os.path.basename(fp))[0] + f\"_trades_q{int(topq*100):02d}_flip.csv\"\n",
    "                trades.to_csv(os.path.join(out_dir, out_name), index=False)\n",
    "                print(f\"[{model_name}] {out_name}: {len(trades)} trades\")\n",
    "            else:\n",
    "                print(f\"[{model_name}] [SKIP] No trades for {os.path.basename(fp)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{model_name}] [ERROR] {os.path.basename(fp)} -> {e}\")\n",
    "\n",
    "    if not all_trades:\n",
    "        print(f\"[{model_name}] No trades aggregated.\")\n",
    "        return None, None\n",
    "\n",
    "    all_trades_df = pd.concat(all_trades, ignore_index=True)\n",
    "    all_trades_df.to_csv(os.path.join(out_dir, f\"ALL_trades_q{int(topq*100):02d}_flip.csv\"), index=False)\n",
    "\n",
    "    summary_h = summarize_trades(all_trades_df)\n",
    "    summary_h.to_csv(os.path.join(out_dir, f\"summary_by_horizon_q{int(topq*100):02d}_flip.csv\"), index=False)\n",
    "\n",
    "    pnl = all_trades_df[\"pnl_units\"]\n",
    "    overall = pd.DataFrame([{\n",
    "        \"n_trades\": int(len(pnl)),\n",
    "        \"hit_rate\": float((pnl > 0).mean()) if len(pnl) else np.nan,\n",
    "        \"avg_pnl\": float(pnl.mean()) if len(pnl) else np.nan,\n",
    "        \"std_pnl\": float(pnl.std(ddof=1)) if len(pnl) > 1 else 0.0,\n",
    "        \"total_pnl\": float(pnl.sum()) if len(pnl) else 0.0,\n",
    "        \"median_pnl\": float(pnl.median()) if len(pnl) else np.nan,\n",
    "        \"sharpe_like\": (pnl.mean() / pnl.std(ddof=1)) if len(pnl) > 1 else np.nan\n",
    "    }])\n",
    "    overall.to_csv(os.path.join(out_dir, f\"summary_overall_q{int(topq*100):02d}_flip.csv\"), index=False)\n",
    "\n",
    "    # Print nicely\n",
    "    print(f\"\\n=== {model_name} — Summary by horizon (q={topq*100:.1f}%, flip-exit) ===\")\n",
    "    print(summary_h.to_string(index=False))\n",
    "    print(f\"\\n=== {model_name} — Overall (q={topq*100:.1f}%, flip-exit) ===\")\n",
    "    print(overall.to_string(index=False))\n",
    "    print(\"\\n\" + \"-\"*90 + \"\\n\")\n",
    "\n",
    "    return summary_h, overall\n",
    "\n",
    "# -------------------- Run both models --------------------\n",
    "def main():\n",
    "    trade_df = load_trade_data(TRADE_XL_PATH)\n",
    "    for model_name, pred_dir, out_dir in MODELS:\n",
    "        run_model(model_name, pred_dir, out_dir, trade_df, TOPQ)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OVERALL (flip-exit, q=10%) =====\n",
      "Counts: LSTM n=302, HMM-LSTM n=302\n",
      "Hit rate: LSTM=0.7815, HMM-LSTM=0.7947, diff=-0.0132, z=-0.398, p=0.6904\n",
      " → HMM-LSTM > LSTM (ns, p=0.6904) \n",
      "Sharpe:  LSTM=0.679116, HMM-LSTM=0.718205, diff=-0.039089, z=-0.551, p=0.5818\n",
      " → HMM-LSTM > LSTM (ns, p=0.5818) \n",
      "\n",
      "===== H= 1 (flip-exit, q=10%) =====\n",
      "Counts: LSTM n=110, HMM-LSTM n=110\n",
      "Hit rate: LSTM=0.7545, HMM-LSTM=0.8455, diff=-0.0909, z=-1.685, p=0.09189\n",
      " → HMM-LSTM > LSTM (ns, p=0.09189) \n",
      "Sharpe:  LSTM=0.569511, HMM-LSTM=0.817593, diff=-0.248082, z=-2.064, p=0.03901\n",
      " → HMM-LSTM > LSTM (significant, p=0.03901) *\n",
      "\n",
      "===== H= 4 (flip-exit, q=10%) =====\n",
      "Counts: LSTM n=98, HMM-LSTM n=98\n",
      "Hit rate: LSTM=0.8061, HMM-LSTM=0.7551, diff=0.0510, z=0.863, p=0.3881\n",
      " → LSTM > HMM-LSTM (ns, p=0.3881) \n",
      "Sharpe:  LSTM=0.676642, HMM-LSTM=0.543215, diff=0.133427, z=1.065, p=0.2869\n",
      " → LSTM > HMM-LSTM (ns, p=0.2869) \n",
      "\n",
      "===== H= 12 (flip-exit, q=10%) =====\n",
      "Counts: LSTM n=94, HMM-LSTM n=94\n",
      "Hit rate: LSTM=0.7872, HMM-LSTM=0.7766, diff=0.0106, z=0.177, p=0.8598\n",
      " → LSTM > HMM-LSTM (ns, p=0.8598) \n",
      "Sharpe:  LSTM=0.815229, HMM-LSTM=0.818702, diff=-0.003474, z=-0.029, p=0.9772\n",
      " → HMM-LSTM > LSTM (ns, p=0.9772) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "# ---- Where your flip-exit outputs live ----\n",
    "ROOT = \"/Users/anwarouni/Downloads/Thesis/Output\"\n",
    "LSTM_DIR = os.path.join(ROOT, \"Trading Backtest (LSTM q10 flip)\")\n",
    "HMM_DIR  = os.path.join(ROOT, \"Trading Backtest (HMM_LSTM q10 flip)\")\n",
    "\n",
    "LSTM_ALL = os.path.join(LSTM_DIR, \"ALL_trades_q10_flip.csv\")\n",
    "HMM_ALL  = os.path.join(HMM_DIR,  \"ALL_trades_q10_flip.csv\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def load_trades(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, parse_dates=[\"entry_date\",\"exit_date\"])\n",
    "    # Ensure required columns exist\n",
    "    need = {\"pnl_units\",\"horizon_w\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing columns in {path}. Need {need}, got {df.columns.tolist()}\")\n",
    "    df = df.sort_values(\"exit_date\").reset_index(drop=True)\n",
    "    df[\"win\"] = (df[\"pnl_units\"] > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "def norm_cdf(z: float) -> float:\n",
    "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "# ----- Hit rate: two-proportion z-test -----\n",
    "def hitrate_test(hits1: int, n1: int, hits2: int, n2: int) -> Tuple[float,float,float]:\n",
    "    \"\"\"Returns (diff = p1-p2, z, p_two_sided)\"\"\"\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    p1 = hits1 / n1\n",
    "    p2 = hits2 / n2\n",
    "    p_pool = (hits1 + hits2) / (n1 + n2)\n",
    "    se = math.sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
    "    z = (p1 - p2) / se if se > 0 else float(\"inf\")\n",
    "    p = 2 * (1 - norm_cdf(abs(z)))\n",
    "    return (p1 - p2), z, p\n",
    "\n",
    "# ----- Sharpe ratio: Jobson–Korkie with Memmel correction -----\n",
    "def sharpe_jk_memmel(r1: np.ndarray, r2: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    r1, r2: aligned per-trade P&L (or returns) arrays (same length).\n",
    "    Returns dict with SR1, SR2, diff, z, p.\n",
    "    Note: This is the same functional form we've been using for consistency.\n",
    "    \"\"\"\n",
    "    r1 = np.asarray(r1, dtype=float)\n",
    "    r2 = np.asarray(r2, dtype=float)\n",
    "    if len(r1) != len(r2):\n",
    "        raise ValueError(\"Sharpe test requires same-length aligned series.\")\n",
    "    n = len(r1)\n",
    "    if n < 2:\n",
    "        return {\"SR1\": np.nan, \"SR2\": np.nan, \"diff\": np.nan, \"z\": np.nan, \"p\": np.nan}\n",
    "\n",
    "    mu1, mu2 = r1.mean(), r2.mean()\n",
    "    s1, s2   = r1.std(ddof=1), r2.std(ddof=1)\n",
    "    SR1, SR2 = mu1 / s1 if s1>0 else np.nan, mu2 / s2 if s2>0 else np.nan\n",
    "    cov12    = np.cov(r1, r2, ddof=1)[0,1]\n",
    "\n",
    "    # Memmel-corrected variance of SR difference (simple, consistent form used before)\n",
    "    var_d = (1.0 / n) * ( 2 * (1 - SR1**2) * (1 - SR2**2) + 2 * (cov12 / (s1 * s2)) )\n",
    "    var_d = max(var_d, 1e-12)\n",
    "    z = (SR1 - SR2) / math.sqrt(var_d)\n",
    "    p = 2 * (1 - norm_cdf(abs(z)))\n",
    "    return {\"SR1\": SR1, \"SR2\": SR2, \"diff\": SR1 - SR2, \"z\": z, \"p\": p}\n",
    "\n",
    "def label_sig(name1: str, name2: str, diff: float, p: float, higher_is_better=True) -> str:\n",
    "    if math.isnan(diff) or math.isnan(p):\n",
    "        return \"n/a\"\n",
    "    stars = \"***\" if p < 0.001 else (\"**\" if p < 0.01 else (\"*\" if p < 0.05 else \"\"))\n",
    "    if diff > 0:\n",
    "        better = f\"{name1} > {name2}\"\n",
    "    elif diff < 0:\n",
    "        better = f\"{name2} > {name1}\"\n",
    "    else:\n",
    "        better = f\"{name1} = {name2}\"\n",
    "    verdict = \"significant\" if p < 0.05 else \"ns\"\n",
    "    return f\"{better} ({verdict}, p={p:.4g}) {stars}\"\n",
    "\n",
    "def analyze_block(df1: pd.DataFrame, df2: pd.DataFrame, mask: pd.Series, label: str, name1=\"LSTM\", name2=\"HMM-LSTM\"):\n",
    "    a = df1[mask]\n",
    "    b = df2[mask]\n",
    "    # Hit-rate test\n",
    "    hits1, n1 = int(a[\"win\"].sum()), int(len(a))\n",
    "    hits2, n2 = int(b[\"win\"].sum()), int(len(b))\n",
    "    diff_hr, z_hr, p_hr = hitrate_test(hits1, n1, hits2, n2)\n",
    "\n",
    "    # Sharpe test: align by exit_date positionally after sorting (both already sorted)\n",
    "    # We take min length to avoid mismatch (should be equal in your outputs).\n",
    "    m = min(len(a), len(b))\n",
    "    r1 = a[\"pnl_units\"].to_numpy()[:m]\n",
    "    r2 = b[\"pnl_units\"].to_numpy()[:m]\n",
    "    sr = sharpe_jk_memmel(r1, r2)\n",
    "\n",
    "    print(f\"\\n===== {label} =====\")\n",
    "    print(f\"Counts: {name1} n={n1}, {name2} n={n2}\")\n",
    "    # Hit rate block\n",
    "    hr1 = hits1 / n1 if n1 else np.nan\n",
    "    hr2 = hits2 / n2 if n2 else np.nan\n",
    "    print(f\"Hit rate: {name1}={hr1:.4f}, {name2}={hr2:.4f}, diff={diff_hr:.4f}, z={z_hr:.3f}, p={p_hr:.4g}\")\n",
    "    print(\" → \" + label_sig(name1, name2, diff_hr, p_hr))\n",
    "    # Sharpe block\n",
    "    print(f\"Sharpe:  {name1}={sr['SR1']:.6f}, {name2}={sr['SR2']:.6f}, diff={sr['diff']:.6f}, z={sr['z']:.3f}, p={sr['p']:.4g}\")\n",
    "    print(\" → \" + label_sig(name1, name2, sr[\"diff\"], sr[\"p\"]))\n",
    "\n",
    "def main():\n",
    "    lstm = load_trades(LSTM_ALL)\n",
    "    hmm  = load_trades(HMM_ALL)\n",
    "\n",
    "    # OVERALL\n",
    "    analyze_block(lstm, hmm, mask=pd.Series([True]*len(lstm)), label=\"OVERALL (flip-exit, q=10%)\")\n",
    "\n",
    "    # PER HORIZON\n",
    "    for H in [1, 4, 12]:\n",
    "        mask = (lstm[\"horizon_w\"] == H)  # both dfs have same horizons counts in your output\n",
    "        analyze_block(lstm, hmm, mask=mask, label=f\"H= {H} (flip-exit, q=10%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARP] arp_split_60_h1_trades_q10_flip.csv: 28 trades\n",
      "[ARP] arp_split_60_h12_trades_q10_flip.csv: 24 trades\n",
      "[ARP] arp_split_60_h4_trades_q10_flip.csv: 26 trades\n",
      "[ARP] arp_split_70_h1_trades_q10_flip.csv: 27 trades\n",
      "[ARP] arp_split_70_h12_trades_q10_flip.csv: 26 trades\n",
      "[ARP] arp_split_70_h4_trades_q10_flip.csv: 26 trades\n",
      "[ARP] arp_split_80_h1_trades_q10_flip.csv: 28 trades\n",
      "[ARP] arp_split_80_h12_trades_q10_flip.csv: 25 trades\n",
      "[ARP] arp_split_80_h4_trades_q10_flip.csv: 21 trades\n",
      "[ARP] arp_split_90_h1_trades_q10_flip.csv: 28 trades\n",
      "[ARP] arp_split_90_h12_trades_q10_flip.csv: 26 trades\n",
      "[ARP] arp_split_90_h4_trades_q10_flip.csv: 26 trades\n",
      "\n",
      "=== ARP — Summary by horizon (q=10.0%, flip-exit) ===\n",
      " horizon_w  n_trades  hit_rate   avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "         1       111  0.459459 -0.000164 0.047487   -0.01815     -0.0039    -0.003443\n",
      "         4        99  0.020202 -0.097722 0.073395   -9.67444     -0.0814    -1.331439\n",
      "        12       101  0.079208 -0.124817 0.116146  -12.60654     -0.0990    -1.074660\n",
      "\n",
      "=== ARP — Overall (q=10.0%, flip-exit) ===\n",
      " n_trades  hit_rate   avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "      311  0.196141 -0.071701 0.099109  -22.29913     -0.0503    -0.723461\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[ARX] arx_split_60_h1_trades_q10_flip.csv: 27 trades\n",
      "[ARX] arx_split_60_h12_trades_q10_flip.csv: 23 trades\n",
      "[ARX] arx_split_60_h4_trades_q10_flip.csv: 26 trades\n",
      "[ARX] arx_split_70_h1_trades_q10_flip.csv: 27 trades\n",
      "[ARX] arx_split_70_h12_trades_q10_flip.csv: 13 trades\n",
      "[ARX] arx_split_70_h4_trades_q10_flip.csv: 26 trades\n",
      "[ARX] arx_split_80_h1_trades_q10_flip.csv: 28 trades\n",
      "[ARX] arx_split_80_h12_trades_q10_flip.csv: 24 trades\n",
      "[ARX] arx_split_80_h4_trades_q10_flip.csv: 21 trades\n",
      "[ARX] arx_split_90_h1_trades_q10_flip.csv: 28 trades\n",
      "[ARX] arx_split_90_h12_trades_q10_flip.csv: 26 trades\n",
      "[ARX] arx_split_90_h4_trades_q10_flip.csv: 26 trades\n",
      "\n",
      "=== ARX — Summary by horizon (q=10.0%, flip-exit) ===\n",
      " horizon_w  n_trades  hit_rate   avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "         1       110  0.327273 -0.025536 0.051157   -2.80901   -0.035475    -0.499175\n",
      "         4        99  0.040404 -0.096110 0.072242   -9.51485   -0.083250    -1.330393\n",
      "        12        86  0.081395 -0.143595 0.137039  -12.34914   -0.099400    -1.047834\n",
      "\n",
      "=== ARX — Overall (q=10.0%, flip-exit) ===\n",
      " n_trades  hit_rate   avg_pnl  std_pnl  total_pnl  median_pnl  sharpe_like\n",
      "      295  0.159322 -0.083637 0.102501    -24.673     -0.0632    -0.815968\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, glob, warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "TOPQ = 0.10  # 10% tails (top and bottom)\n",
    "TRADE_XL_PATH = \"/Users/anwarouni/Downloads/Thesis/Data/TradeData.xlsx\"\n",
    "\n",
    "MODELS = [\n",
    "    (\"ARP\", \"/Users/anwarouni/Downloads/Thesis/Output/AR(p) predictions\",\n",
    "            \"/Users/anwarouni/Downloads/Thesis/Output/Trading Backtest (ARP q10 flip)\"),\n",
    "    (\"ARX\", \"/Users/anwarouni/Downloads/Thesis/Output/AR-X predictions\",\n",
    "            \"/Users/anwarouni/Downloads/Thesis/Output/Trading Backtest (ARX q10 flip)\"),\n",
    "]\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def load_trade_data(xl_path: str) -> pd.DataFrame:\n",
    "    path = xl_path\n",
    "    if (not os.path.exists(path)) and xl_path.endswith(\".xslx\"):\n",
    "        alt = xl_path[:-5] + \"xlsx\"\n",
    "        if os.path.exists(alt): path = alt\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Trade data not found at: {xl_path}\")\n",
    "\n",
    "    try: df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    except Exception: df = pd.read_excel(path)\n",
    "\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    date_col = next((cols[k] for k in cols if k in (\"date\",\"dates\")), None)\n",
    "    swap_col = next((cols[k] for k in cols if k in (\"10yswap\",\"swap\",\"swap10\",\"eur_swap_10y\")), None)\n",
    "    bund_col = next((cols[k] for k in cols if k in (\"10ybund\",\"bund\",\"bund10\",\"de_bund_10y\",\"10yboon\",\"10yboond\",\"10yboonD\")), None)\n",
    "    if not date_col or not swap_col or not bund_col:\n",
    "        raise ValueError(f\"Expected columns like Date, 10Yswap, 10Ybund. Found: {list(df.columns)}\")\n",
    "\n",
    "    df = df[[date_col, swap_col, bund_col]].copy()\n",
    "    df.rename(columns={date_col:\"date\", swap_col:\"swap_10y\", bund_col:\"bund_10y\"}, inplace=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df.sort_values(\"date\", inplace=True)\n",
    "    df[\"spread\"] = df[\"swap_10y\"] - df[\"bund_10y\"]\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def detect_cols(pred: pd.DataFrame) -> Tuple[str, Optional[str], Optional[str]]:\n",
    "    lower = {c.lower(): c for c in pred.columns}\n",
    "    date_col = next((lower[k] for k in lower if k in (\"date\",\"ds\",\"timestamp\",\"time\")), None)\n",
    "    if date_col is None: raise ValueError(f\"No date-like column in predictions: {list(pred.columns)}\")\n",
    "    pred_cands = [\"y_pred\",\"yhat\",\"y_hat\",\"pred\",\"forecast\",\"yhat_level\",\"y_pred_level\"]\n",
    "    y_pred_col = next((lower[k] for k in lower if k in pred_cands), None)\n",
    "    true_cands = [\"y_true\",\"y\",\"y_level\",\"target\",\"actual\",\"truth\"]\n",
    "    y_true_col = next((lower[k] for k in lower if k in true_cands), None)\n",
    "    return date_col, y_true_col, y_pred_col\n",
    "\n",
    "def horizon_from_filename(fname: str) -> Optional[int]:\n",
    "    # Accept ..._H1.csv or ..._h1.csv\n",
    "    m = re.search(r\"_[Hh](1|4|12)\\.csv$\", os.path.basename(fname))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def split_from_filename(fname: str) -> Optional[int]:\n",
    "    # Accept ..._split_60_... or ..._split__60_...\n",
    "    m = re.search(r\"_split_+(\\d+)_\", os.path.basename(fname))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def find_pred_col(pred: pd.DataFrame, H: Optional[int]) -> Optional[str]:\n",
    "    cols = list(pred.columns)\n",
    "    # try horizon-specific first (upper & lower 'h')\n",
    "    horizon_candidates = []\n",
    "    if H is not None:\n",
    "        horizon_candidates += [f\"yhat_H{H}\", f\"y_pred_H{H}\", f\"forecast_H{H}\",\n",
    "                               f\"yhat_h{H}\", f\"y_pred_h{H}\", f\"forecast_h{H}\"]\n",
    "    generic = [\"yhat\",\"y_pred\",\"forecast\",\"pred\",\"prediction\"]\n",
    "    for c in horizon_candidates + generic:\n",
    "        if c in cols: return c\n",
    "        # loose match\n",
    "        for col in cols:\n",
    "            if re.fullmatch(c, col, flags=re.IGNORECASE):\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def pick_quantiles(df_sig: pd.DataFrame, col: str, q: float) -> pd.DataFrame:\n",
    "    lo_thr = df_sig[col].quantile(q); hi_thr = df_sig[col].quantile(1 - q)\n",
    "    sel = df_sig[(df_sig[col] <= lo_thr) | (df_sig[col] >= hi_thr)].copy()\n",
    "    sel[\"signal\"] = np.where(sel[col] >= hi_thr, +1, -1)\n",
    "    return sel\n",
    "\n",
    "# -------- Exit rule: opposite signal --------\n",
    "def find_exit_on_flip(pred_series: pd.Series, entry_idx: int, direction: int) -> Optional[int]:\n",
    "    for i in range(entry_idx + 1, len(pred_series)):\n",
    "        val = pred_series.iloc[i]\n",
    "        if direction == +1 and val < 0: return i\n",
    "        if direction == -1 and val > 0: return i\n",
    "    return None\n",
    "\n",
    "def backtest_one(pred_csv: str, trade_df: pd.DataFrame, topq: float) -> pd.DataFrame:\n",
    "    H = horizon_from_filename(pred_csv)\n",
    "    split = split_from_filename(pred_csv)\n",
    "\n",
    "    if H is None:\n",
    "        # Skip non-horizon files (e.g., \"arx_all_predictions.csv\")\n",
    "        # print(f\"[SKIP] Could not parse horizon from {os.path.basename(pred_csv)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    pred = pd.read_csv(pred_csv)\n",
    "    date_col, y_true_col, y_pred_col = detect_cols(pred)\n",
    "    pred[date_col] = pd.to_datetime(pred[date_col]); pred.sort_values(by=date_col, inplace=True)\n",
    "\n",
    "    if y_true_col is None:\n",
    "        pred = pred.merge(trade_df[\"spread\"].rename(\"y_true\"), left_on=date_col, right_index=True, how=\"left\")\n",
    "        y_true_col = \"y_true\"\n",
    "\n",
    "    if y_pred_col is None:\n",
    "        y_pred_col = find_pred_col(pred, H)\n",
    "        if y_pred_col is None:\n",
    "            raise ValueError(f\"No prediction column found in {os.path.basename(pred_csv)}. Columns={list(pred.columns)}\")\n",
    "\n",
    "    pred[\"pred_delta\"] = pred[y_pred_col] - pred[y_true_col]\n",
    "    pred = pred[pred[date_col].isin(trade_df.index)]\n",
    "    if pred.empty: return pd.DataFrame()\n",
    "\n",
    "    sig = pick_quantiles(pred[[date_col, \"pred_delta\"]].copy(), \"pred_delta\", q=topq)\n",
    "\n",
    "    # Align to trading calendar and ffill predictions\n",
    "    pred_aligned = (\n",
    "        pd.DataFrame({\"date_idx\": trade_df.index})\n",
    "        .merge(pred[[date_col, \"pred_delta\"]], left_on=\"date_idx\", right_on=date_col, how=\"left\")\n",
    "        .set_index(\"date_idx\")[\"pred_delta\"]\n",
    "        .ffill()\n",
    "    )\n",
    "\n",
    "    entries = sig[date_col].values\n",
    "    idx_pos = trade_df.index.get_indexer(entries)\n",
    "\n",
    "    exits_idx, reasons = [], []\n",
    "    for pos, direction in zip(idx_pos, sig[\"signal\"].values):\n",
    "        if pos == -1:\n",
    "            exits_idx.append(pd.NaT); reasons.append(\"no_entry_match\"); continue\n",
    "        exit_pos = find_exit_on_flip(pred_aligned, pos, int(direction))\n",
    "        if exit_pos is None or exit_pos >= len(trade_df.index):\n",
    "            exits_idx.append(pd.NaT); reasons.append(\"no_exit\")\n",
    "        else:\n",
    "            exits_idx.append(trade_df.index[exit_pos]); reasons.append(\"flip_exit\")\n",
    "\n",
    "    trades = pd.DataFrame({\n",
    "        \"entry_date\": entries,\n",
    "        \"exit_date\": exits_idx,\n",
    "        \"signal\": sig[\"signal\"].values,\n",
    "        \"pred_delta_at_entry\": sig[\"pred_delta\"].values,\n",
    "        \"exit_reason\": reasons\n",
    "    }).dropna(subset=[\"exit_date\"]).copy()\n",
    "\n",
    "    entry_spread = trade_df.loc[trades[\"entry_date\"], \"spread\"].values\n",
    "    exit_spread  = trade_df.loc[trades[\"exit_date\"],  \"spread\"].values\n",
    "    trades[\"realized_delta\"] = exit_spread - entry_spread\n",
    "    trades[\"pnl_units\"] = trades[\"signal\"] * trades[\"realized_delta\"]\n",
    "\n",
    "    trades[\"horizon_w\"] = H\n",
    "    trades[\"split_pct\"] = split\n",
    "    trades[\"src_file\"] = os.path.basename(pred_csv)\n",
    "    return trades\n",
    "\n",
    "def summarize_trades(trades: pd.DataFrame) -> pd.DataFrame:\n",
    "    if trades.empty: return pd.DataFrame()\n",
    "    grp = trades.groupby([\"horizon_w\"]).agg(\n",
    "        n_trades=(\"pnl_units\",\"count\"),\n",
    "        hit_rate=(\"pnl_units\", lambda x: np.mean(x>0)),\n",
    "        avg_pnl=(\"pnl_units\",\"mean\"),\n",
    "        std_pnl=(\"pnl_units\", lambda x: float(np.std(x, ddof=1)) if len(x)>1 else 0.0),\n",
    "        total_pnl=(\"pnl_units\",\"sum\"),\n",
    "        median_pnl=(\"pnl_units\",\"median\")\n",
    "    ).reset_index()\n",
    "    grp[\"sharpe_like\"] = grp.apply(\n",
    "        lambda r: (r[\"avg_pnl\"] / r[\"std_pnl\"]) if r[\"std_pnl\"] not in (0.0, np.nan) else np.nan, axis=1\n",
    "    )\n",
    "    return grp\n",
    "\n",
    "def run_model(model_name: str, pred_dir: str, out_dir: str, trade_df: pd.DataFrame, topq: float):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Grab all CSVs; let backtest_one decide which to skip based on horizon\n",
    "    csvs = sorted(glob.glob(os.path.join(pred_dir, \"*.csv\")))\n",
    "    if not csvs:\n",
    "        print(f\"[{model_name}] No .csv files found in {pred_dir}\")\n",
    "        return None, None\n",
    "\n",
    "    all_trades = []\n",
    "    for fp in csvs:\n",
    "        try:\n",
    "            trades = backtest_one(fp, trade_df, topq=topq)\n",
    "            if not trades.empty:\n",
    "                all_trades.append(trades)\n",
    "                out_name = os.path.splitext(os.path.basename(fp))[0] + f\"_trades_q{int(topq*100):02d}_flip.csv\"\n",
    "                trades.to_csv(os.path.join(out_dir, out_name), index=False)\n",
    "                print(f\"[{model_name}] {out_name}: {len(trades)} trades\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{model_name}] [ERROR] {os.path.basename(fp)} -> {e}\")\n",
    "\n",
    "    if not all_trades:\n",
    "        print(f\"[{model_name}] No trades aggregated in {pred_dir}.\")\n",
    "        return None, None\n",
    "\n",
    "    all_trades_df = pd.concat(all_trades, ignore_index=True)\n",
    "    all_trades_df.to_csv(os.path.join(out_dir, f\"ALL_trades_q{int(topq*100):02d}_flip.csv\"), index=False)\n",
    "\n",
    "    summary_h = summarize_trades(all_trades_df)\n",
    "    summary_h.to_csv(os.path.join(out_dir, f\"summary_by_horizon_q{int(topq*100):02d}_flip.csv\"), index=False)\n",
    "\n",
    "    pnl = all_trades_df[\"pnl_units\"]\n",
    "    overall = pd.DataFrame([{\n",
    "        \"n_trades\": int(len(pnl)),\n",
    "        \"hit_rate\": float((pnl > 0).mean()) if len(pnl) else np.nan,\n",
    "        \"avg_pnl\": float(pnl.mean()) if len(pnl) else np.nan,\n",
    "        \"std_pnl\": float(pnl.std(ddof=1)) if len(pnl) > 1 else 0.0,\n",
    "        \"total_pnl\": float(pnl.sum()) if len(pnl) else 0.0,\n",
    "        \"median_pnl\": float(pnl.median()) if len(pnl) else np.nan,\n",
    "        \"sharpe_like\": (pnl.mean() / pnl.std(ddof=1)) if len(pnl) > 1 and pnl.std(ddof=1) != 0 else np.nan\n",
    "    }])\n",
    "    overall.to_csv(os.path.join(out_dir, f\"summary_overall_q{int(topq*100):02d}_flip.csv\"), index=False)\n",
    "\n",
    "    # Print nicely\n",
    "    print(f\"\\n=== {model_name} — Summary by horizon (q={topq*100:.1f}%, flip-exit) ===\")\n",
    "    print(summary_h.to_string(index=False))\n",
    "    print(f\"\\n=== {model_name} — Overall (q={topq*100:.1f}%, flip-exit) ===\")\n",
    "    print(overall.to_string(index=False))\n",
    "    print(\"\\n\" + \"-\"*90 + \"\\n\")\n",
    "\n",
    "    return summary_h, overall\n",
    "\n",
    "# -------------------- Run both models --------------------\n",
    "def main():\n",
    "    trade_df = load_trade_data(TRADE_XL_PATH)\n",
    "    for model_name, pred_dir, out_dir in MODELS:\n",
    "        run_model(model_name, pred_dir, out_dir, trade_df, TOPQ)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OVERALL (flip-exit, q=10%) — HMM-LSTM vs AR(p) =====\n",
      "Counts: HMM-LSTM n=302, AR(p) n=311\n",
      "Hit rate: HMM-LSTM=0.7947, AR(p)=0.1961, diff=0.5986, z=14.820, p=0\n",
      " → HMM-LSTM > AR(p) (significant, p=0) ***\n",
      "Sharpe:  HMM-LSTM=0.718205, AR(p)=-0.729953, diff=1.448158, z=76.876, p=0\n",
      " → HMM-LSTM > AR(p) (significant, p=0) ***\n",
      "\n",
      "===== H=1 (flip-exit, q=10%) — HMM-LSTM vs AR(p) =====\n",
      "Counts: HMM-LSTM n=110, AR(p) n=111\n",
      "Hit rate: HMM-LSTM=0.8000, AR(p)=0.4595, diff=0.3405, z=5.240, p=1.608e-07\n",
      " → HMM-LSTM > AR(p) (significant, p=1.608e-07) ***\n",
      "Sharpe:  HMM-LSTM=0.666037, AR(p)=0.004245, diff=0.661792, z=6.706, p=2.003e-11\n",
      " → HMM-LSTM > AR(p) (significant, p=2.003e-11) ***\n",
      "\n",
      "===== H=4 (flip-exit, q=10%) — HMM-LSTM vs AR(p) =====\n",
      "Counts: HMM-LSTM n=98, AR(p) n=99\n",
      "Hit rate: HMM-LSTM=0.7857, AR(p)=0.0202, diff=0.7655, z=10.961, p=0\n",
      " → HMM-LSTM > AR(p) (significant, p=0) ***\n",
      "Sharpe:  HMM-LSTM=0.669254, AR(p)=-1.346678, diff=2.015932, z=2015931.724, p=0\n",
      " → HMM-LSTM > AR(p) (significant, p=0) ***\n",
      "\n",
      "===== H=12 (flip-exit, q=10%) — HMM-LSTM vs AR(p) =====\n",
      "Counts: HMM-LSTM n=94, AR(p) n=101\n",
      "Hit rate: HMM-LSTM=0.7979, AR(p)=0.0792, diff=0.7187, z=10.142, p=0\n",
      " → HMM-LSTM > AR(p) (significant, p=0) ***\n",
      "Sharpe:  HMM-LSTM=0.831137, AR(p)=-1.015167, diff=1.846304, z=1846303.672, p=0\n",
      " → HMM-LSTM > AR(p) (significant, p=0) ***\n",
      "\n",
      "===== OVERALL (flip-exit, q=10%) — HMM-LSTM vs AR-X =====\n",
      "Counts: HMM-LSTM n=302, AR-X n=295\n",
      "Hit rate: HMM-LSTM=0.7947, AR-X=0.1593, diff=0.6354, z=15.535, p=0\n",
      " → HMM-LSTM > AR-X (significant, p=0) ***\n",
      "Sharpe:  HMM-LSTM=0.705458, AR-X=-0.815968, diff=1.521426, z=34.994, p=0\n",
      " → HMM-LSTM > AR-X (significant, p=0) ***\n",
      "\n",
      "===== H=1 (flip-exit, q=10%) — HMM-LSTM vs AR-X =====\n",
      "Counts: HMM-LSTM n=110, AR-X n=110\n",
      "Hit rate: HMM-LSTM=0.8000, AR-X=0.3273, diff=0.4727, z=7.069, p=1.559e-12\n",
      " → HMM-LSTM > AR-X (significant, p=1.559e-12) ***\n",
      "Sharpe:  HMM-LSTM=0.666037, AR-X=-0.499175, diff=1.165212, z=11.886, p=0\n",
      " → HMM-LSTM > AR-X (significant, p=0) ***\n",
      "\n",
      "===== H=4 (flip-exit, q=10%) — HMM-LSTM vs AR-X =====\n",
      "Counts: HMM-LSTM n=98, AR-X n=99\n",
      "Hit rate: HMM-LSTM=0.7857, AR-X=0.0404, diff=0.7453, z=10.630, p=0\n",
      " → HMM-LSTM > AR-X (significant, p=0) ***\n",
      "Sharpe:  HMM-LSTM=0.669254, AR-X=-1.345541, diff=2.014795, z=2014794.878, p=0\n",
      " → HMM-LSTM > AR-X (significant, p=0) ***\n",
      "\n",
      "===== H=12 (flip-exit, q=10%) — HMM-LSTM vs AR-X =====\n",
      "Counts: HMM-LSTM n=94, AR-X n=86\n",
      "Hit rate: HMM-LSTM=0.7979, AR-X=0.0814, diff=0.7165, z=9.641, p=0\n",
      " → HMM-LSTM > AR-X (significant, p=0) ***\n",
      "Sharpe:  HMM-LSTM=0.820096, AR-X=-1.047834, diff=1.867930, z=1867929.857, p=0\n",
      " → HMM-LSTM > AR-X (significant, p=0) ***\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# ---- Where your flip-exit outputs live ----\n",
    "ROOT     = \"/Users/anwarouni/Downloads/Thesis/Output\"\n",
    "HMM_DIR  = os.path.join(ROOT, \"Trading Backtest (HMM_LSTM q10 flip)\")\n",
    "ARP_DIR  = os.path.join(ROOT, \"Trading Backtest (ARP q10 flip)\")\n",
    "ARX_DIR  = os.path.join(ROOT, \"Trading Backtest (ARX q10 flip)\")\n",
    "\n",
    "HMM_ALL  = os.path.join(HMM_DIR, \"ALL_trades_q10_flip.csv\")\n",
    "ARP_ALL  = os.path.join(ARP_DIR, \"ALL_trades_q10_flip.csv\")\n",
    "ARX_ALL  = os.path.join(ARX_DIR, \"ALL_trades_q10_flip.csv\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def load_trades(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, parse_dates=[\"entry_date\",\"exit_date\"])\n",
    "    need = {\"pnl_units\",\"horizon_w\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing columns in {path}. Need {need}, got {df.columns.tolist()}\")\n",
    "    df = df.sort_values(\"exit_date\").reset_index(drop=True)\n",
    "    df[\"win\"] = (df[\"pnl_units\"] > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "def norm_cdf(z: float) -> float:\n",
    "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "# ----- Hit rate: two-proportion z-test (independent samples) -----\n",
    "def hitrate_test(hits1: int, n1: int, hits2: int, n2: int) -> Tuple[float,float,float]:\n",
    "    \"\"\"Returns (diff = p1-p2, z, p_two_sided)\"\"\"\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    p1 = hits1 / n1\n",
    "    p2 = hits2 / n2\n",
    "    p_pool = (hits1 + hits2) / (n1 + n2)\n",
    "    se = math.sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
    "    z = (p1 - p2) / se if se > 0 else float(\"inf\")\n",
    "    p = 2 * (1 - norm_cdf(abs(z)))\n",
    "    return (p1 - p2), z, p\n",
    "\n",
    "# ----- Sharpe ratio: Jobson–Korkie with Memmel correction -----\n",
    "def sharpe_jk_memmel(r1: np.ndarray, r2: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    r1, r2: aligned per-trade P&L arrays (same length after trimming to min length).\n",
    "    Returns dict with SR1, SR2, diff, z, p.\n",
    "    \"\"\"\n",
    "    r1 = np.asarray(r1, dtype=float)\n",
    "    r2 = np.asarray(r2, dtype=float)\n",
    "    n = len(r1)\n",
    "    if n < 2 or len(r2) != n:\n",
    "        return {\"SR1\": np.nan, \"SR2\": np.nan, \"diff\": np.nan, \"z\": np.nan, \"p\": np.nan}\n",
    "\n",
    "    mu1, mu2 = r1.mean(), r2.mean()\n",
    "    s1, s2   = r1.std(ddof=1), r2.std(ddof=1)\n",
    "    SR1, SR2 = (mu1 / s1 if s1 > 0 else np.nan), (mu2 / s2 if s2 > 0 else np.nan)\n",
    "    cov12    = np.cov(r1, r2, ddof=1)[0,1]\n",
    "\n",
    "    # Memmel-corrected variance of SR difference (compact form used previously)\n",
    "    var_d = (1.0 / n) * ( 2 * (1 - SR1**2) * (1 - SR2**2) + 2 * (cov12 / (s1 * s2)) )\n",
    "    var_d = max(var_d, 1e-12)\n",
    "    z = (SR1 - SR2) / math.sqrt(var_d)\n",
    "    p = 2 * (1 - norm_cdf(abs(z)))\n",
    "    return {\"SR1\": SR1, \"SR2\": SR2, \"diff\": SR1 - SR2, \"z\": z, \"p\": p}\n",
    "\n",
    "def label_sig(name1: str, name2: str, diff: float, p: float) -> str:\n",
    "    if math.isnan(diff) or math.isnan(p):\n",
    "        return \"n/a\"\n",
    "    stars = \"***\" if p < 1e-3 else (\"**\" if p < 1e-2 else (\"*\" if p < 5e-2 else \"\"))\n",
    "    if diff > 0:\n",
    "        better = f\"{name1} > {name2}\"\n",
    "    elif diff < 0:\n",
    "        better = f\"{name2} > {name1}\"\n",
    "    else:\n",
    "        better = f\"{name1} = {name2}\"\n",
    "    verdict = \"significant\" if p < 0.05 else \"ns\"\n",
    "    return f\"{better} ({verdict}, p={p:.4g}) {stars}\"\n",
    "\n",
    "def analyze_block(df1: pd.DataFrame, df2: pd.DataFrame, label: str,\n",
    "                  name1: str, name2: str, horizon: Optional[int] = None):\n",
    "    a = df1.copy()\n",
    "    b = df2.copy()\n",
    "    if horizon is not None:\n",
    "        a = a[a[\"horizon_w\"] == horizon]\n",
    "        b = b[b[\"horizon_w\"] == horizon]\n",
    "\n",
    "    # Hit-rate test\n",
    "    hits1, n1 = int(a[\"win\"].sum()), int(len(a))\n",
    "    hits2, n2 = int(b[\"win\"].sum()), int(len(b))\n",
    "    diff_hr, z_hr, p_hr = hitrate_test(hits1, n1, hits2, n2)\n",
    "\n",
    "    # Sharpe test: align per-trade PnL by time order; trim to min length\n",
    "    m = min(len(a), len(b))\n",
    "    r1 = a[\"pnl_units\"].to_numpy()[:m]\n",
    "    r2 = b[\"pnl_units\"].to_numpy()[:m]\n",
    "    sr = sharpe_jk_memmel(r1, r2)\n",
    "\n",
    "    print(f\"\\n===== {label} — {name1} vs {name2} =====\")\n",
    "    print(f\"Counts: {name1} n={n1}, {name2} n={n2}\")\n",
    "    # Hit rate\n",
    "    hr1 = hits1 / n1 if n1 else np.nan\n",
    "    hr2 = hits2 / n2 if n2 else np.nan\n",
    "    print(f\"Hit rate: {name1}={hr1:.4f}, {name2}={hr2:.4f}, diff={diff_hr:.4f}, z={z_hr:.3f}, p={p_hr:.4g}\")\n",
    "    print(\" → \" + label_sig(name1, name2, diff_hr, p_hr))\n",
    "    # Sharpe\n",
    "    print(f\"Sharpe:  {name1}={sr['SR1']:.6f}, {name2}={sr['SR2']:.6f}, diff={sr['diff']:.6f}, z={sr['z']:.3f}, p={sr['p']:.4g}\")\n",
    "    print(\" → \" + label_sig(name1, name2, sr['diff'], sr['p']))\n",
    "\n",
    "def compare_pair(name1: str, path1: str, name2: str, path2: str):\n",
    "    df1 = load_trades(path1)\n",
    "    df2 = load_trades(path2)\n",
    "\n",
    "    # OVERALL\n",
    "    analyze_block(df1, df2, label=\"OVERALL (flip-exit, q=10%)\", name1=name1, name2=name2, horizon=None)\n",
    "    # PER HORIZON\n",
    "    for H in (1, 4, 12):\n",
    "        analyze_block(df1, df2, label=f\"H={H} (flip-exit, q=10%)\", name1=name1, name2=name2, horizon=H)\n",
    "\n",
    "def main():\n",
    "    # HMM_LSTM vs AR(p)\n",
    "    compare_pair(\"HMM-LSTM\", HMM_ALL, \"AR(p)\", ARP_ALL)\n",
    "    # HMM_LSTM vs AR-X\n",
    "    compare_pair(\"HMM-LSTM\", HMM_ALL, \"AR-X\", ARX_ALL)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weekly Overlap vs HMM_LSTM (grid: W-FRI, window=1w) ===\n",
      "   model  weeks_in_base  weeks_in_model  weeks_both  weeks_either  jaccard_overlap_pct  conditional_overlap_pct  median_overlap_run_weeks  median_nonoverlap_run_weeks\n",
      "     ARP            378             430         315           493                63.89                    83.33                         5                            3\n",
      "     ARX            378             384         301           461                65.29                    79.63                         6                            2\n",
      "HMM_LSTM            378             378         378           378               100.00                   100.00                         9                            0\n",
      "    LSTM            378             383         375           386                97.15                    99.21                         9                            1\n",
      "\n",
      "=== Entry Alignment vs HMM_LSTM (±1w) ===\n",
      "   model  n_entry_base  n_entry_model  entry_aligned_pct  entry_matched_gap_median_w  entry_matched_gap_mean_w  entry_unmatched_count  entry_unmatched_nearest_gap_median_w  entry_unmatched_nearest_gap_mean_w\n",
      "     ARP           302            311              56.95                        0.00                      0.37                    130                                  0.00                               -0.47\n",
      "     ARX           302            295              57.62                        0.00                      0.10                    128                                 -1.00                               -0.40\n",
      "HMM_LSTM           302            302             100.00                        0.00                      0.00                      0                                   NaN                                 NaN\n",
      "    LSTM           302            302              96.36                        0.00                      0.06                     11                                  0.00                               -0.18\n",
      "\n",
      "=== Exit Alignment vs HMM_LSTM (±1w) ===\n",
      "   model  n_exit_base  n_exit_model  exit_aligned_pct  exit_matched_gap_median_w  exit_matched_gap_mean_w  exit_unmatched_count  exit_unmatched_nearest_gap_median_w  exit_unmatched_nearest_gap_mean_w\n",
      "     ARP          302           311             42.72                       0.00                     0.11                   173                                -1.00                              -0.88\n",
      "     ARX          302           295             44.04                       0.00                    -0.14                   169                                -1.00                              -0.55\n",
      "HMM_LSTM          302           302            100.00                       0.00                     0.00                     0                                  NaN                                NaN\n",
      "    LSTM          302           302             95.03                       0.00                     0.01                    15                                 0.00                               1.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "TRADE_FOLDER = \"/Users/anwarouni/Downloads/Thesis/Output\"\n",
    "MODELS = [\"ARP\", \"ARX\", \"LSTM\", \"HMM_LSTM\"]\n",
    "TOPQ = 0.10\n",
    "\n",
    "# Weekly grid + event matching\n",
    "WEEK_FREQ = \"W-FRI\"             # pick a consistent week anchor; change to W-MON if you prefer\n",
    "MATCH_WINDOW_WEEKS = 1          # entries/exits within ±1 week are considered \"aligned\"\n",
    "\n",
    "# Output\n",
    "OUTDIR = Path(\"/Users/anwarouni/Downloads/Thesis/Output/TradeAlignment\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def load_trades(model: str) -> pd.DataFrame:\n",
    "    model_dir = Path(TRADE_FOLDER) / f\"Trading Backtest ({model} q{int(TOPQ*100)} flip)\"\n",
    "    f = model_dir / f\"ALL_trades_q{int(TOPQ*100)}_flip.csv\"\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {f}\")\n",
    "    df = pd.read_csv(f, parse_dates=[\"entry_date\", \"exit_date\"]).sort_values(\"entry_date\").reset_index(drop=True)\n",
    "    # defensive: ensure exit after entry\n",
    "    df = df[df[\"exit_date\"] >= df[\"entry_date\"]].copy()\n",
    "    return df\n",
    "\n",
    "def weekly_inmarket_series(df: pd.DataFrame, week_index: pd.DatetimeIndex) -> pd.Series:\n",
    "    \"\"\"Return boolean Series over week_index: True if in market that week.\"\"\"\n",
    "    s = pd.Series(False, index=week_index)\n",
    "    if df.empty:\n",
    "        return s\n",
    "    # mark weeks covered by each trade\n",
    "    for _, r in df.iterrows():\n",
    "        # cover inclusive bounds on this weekly grid\n",
    "        rng = pd.date_range(r[\"entry_date\"], r[\"exit_date\"], freq=WEEK_FREQ)\n",
    "        s.loc[s.index.isin(rng)] = True\n",
    "    return s\n",
    "\n",
    "def segment_lengths(mask: pd.Series, value: bool) -> list[int]:\n",
    "    \"\"\"Lengths (in weeks) of consecutive segments where mask == value.\"\"\"\n",
    "    if mask.empty:\n",
    "        return []\n",
    "    runs = []\n",
    "    run = 0\n",
    "    for v in mask.values:\n",
    "        if v == value:\n",
    "            run += 1\n",
    "        else:\n",
    "            if run > 0:\n",
    "                runs.append(run)\n",
    "            run = 0\n",
    "    if run > 0:\n",
    "        runs.append(run)\n",
    "    return runs\n",
    "\n",
    "def match_events(a_dates: pd.Series, b_dates: pd.Series, window_weeks: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Greedy nearest-neighbour matching of dates in a_dates to b_dates within ±window_weeks.\n",
    "    Returns a DataFrame with columns: a_date, b_date (or NaT), gap_weeks (signed, b - a).\n",
    "    \"\"\"\n",
    "    a_dates = pd.to_datetime(a_dates).sort_values().reset_index(drop=True)\n",
    "    b_dates = pd.to_datetime(b_dates).sort_values().reset_index(drop=True)\n",
    "    used = np.zeros(len(b_dates), dtype=bool)\n",
    "\n",
    "    rows = []\n",
    "    for a in a_dates:\n",
    "        # find nearest b (by absolute week diff) that is unused and within window\n",
    "        if b_dates.empty:\n",
    "            rows.append({\"a_date\": a, \"b_date\": pd.NaT, \"gap_weeks\": np.nan})\n",
    "            continue\n",
    "        diffs = (b_dates - a).dt.days / 7.0\n",
    "        diffs_abs = diffs.abs()\n",
    "        # set used to inf so they won't be picked\n",
    "        diffs_abs.values[used] = np.inf\n",
    "        j = diffs_abs.values.argmin()\n",
    "        best_gap = diffs.values[j]\n",
    "        if np.isfinite(diffs_abs.values[j]) and abs(best_gap) <= window_weeks:\n",
    "            used[j] = True\n",
    "            rows.append({\"a_date\": a, \"b_date\": b_dates.iloc[j], \"gap_weeks\": best_gap})\n",
    "        else:\n",
    "            rows.append({\"a_date\": a, \"b_date\": pd.NaT, \"gap_weeks\": np.nan})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def nearest_gap_for_unmatched(unmatched_dates: pd.Series, other_dates: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    For each unmatched date, compute signed gap (in weeks) to nearest date in other_dates.\n",
    "    Positive means other is after, negative means before.\n",
    "    \"\"\"\n",
    "    unmatched_dates = pd.to_datetime(unmatched_dates).sort_values().reset_index(drop=True)\n",
    "    other_dates = pd.to_datetime(other_dates).sort_values().reset_index(drop=True)\n",
    "    if unmatched_dates.empty or other_dates.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    gaps = []\n",
    "    for d in unmatched_dates:\n",
    "        diffs = (other_dates - d).dt.days / 7.0\n",
    "        j = diffs.abs().values.argmin()\n",
    "        gaps.append(diffs.iloc[j])\n",
    "    return pd.Series(gaps, name=\"nearest_gap_weeks\")\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "def main():\n",
    "    # Load trades\n",
    "    trades = {m: load_trades(m) for m in MODELS}\n",
    "    base = \"HMM_LSTM\"\n",
    "    assert base in trades, \"HMM_LSTM trades are required as the reference.\"\n",
    "\n",
    "    # Build a global weekly index covering all models\n",
    "    min_date = min(df[\"entry_date\"].min() for df in trades.values() if not df.empty)\n",
    "    max_date = max(df[\"exit_date\"].max()  for df in trades.values() if not df.empty)\n",
    "    week_index = pd.date_range(min_date, max_date, freq=WEEK_FREQ)\n",
    "\n",
    "    # Weekly in-market series per model\n",
    "    weekly = {m: weekly_inmarket_series(trades[m], week_index) for m in MODELS}\n",
    "\n",
    "    # ---- Weekly overlap summaries vs HMM_LSTM ----\n",
    "    weekly_rows = []\n",
    "    base_s = weekly[base]\n",
    "    for m in MODELS:\n",
    "        s = weekly[m]\n",
    "        both = base_s & s\n",
    "        either = base_s | s\n",
    "        jaccard = (both.sum() / max(1, either.sum())) * 100.0\n",
    "        cond_on_base = (both.sum() / max(1, base_s.sum())) * 100.0\n",
    "\n",
    "        non_overlap = either & (~both)            # weeks where at least one is in-market but not both\n",
    "        both_runs  = segment_lengths(both, True)\n",
    "        non_runs   = segment_lengths(non_overlap, True)\n",
    "\n",
    "        weekly_rows.append({\n",
    "            \"model\": m,\n",
    "            \"weeks_in_base\": int(base_s.sum()),\n",
    "            \"weeks_in_model\": int(s.sum()),\n",
    "            \"weeks_both\": int(both.sum()),\n",
    "            \"weeks_either\": int(either.sum()),\n",
    "            \"jaccard_overlap_pct\": jaccard,\n",
    "            \"conditional_overlap_pct\": cond_on_base,\n",
    "            \"median_overlap_run_weeks\": int(np.median(both_runs)) if both_runs else 0,\n",
    "            \"median_nonoverlap_run_weeks\": int(np.median(non_runs)) if non_runs else 0,\n",
    "        })\n",
    "\n",
    "    weekly_df = pd.DataFrame(weekly_rows).sort_values(\"model\")\n",
    "    weekly_df.to_csv(OUTDIR / \"weekly_overlap_vs_HMM_LSTM.csv\", index=False)\n",
    "\n",
    "    # ---- Event-level alignment (entries & exits), vs HMM_LSTM ----\n",
    "    def summarize_event_alignment(event: str):\n",
    "        rows = []\n",
    "        base_dates = trades[base][f\"{event}_date\"]\n",
    "\n",
    "        for m in MODELS:\n",
    "            other_dates = trades[m][f\"{event}_date\"]\n",
    "\n",
    "            match_df = match_events(base_dates, other_dates, MATCH_WINDOW_WEEKS)\n",
    "            matched = match_df[\"b_date\"].notna()\n",
    "            pct_aligned = 100.0 * matched.mean()\n",
    "\n",
    "            # gaps for matched pairs\n",
    "            gaps_matched = match_df.loc[matched, \"gap_weeks\"].astype(float)\n",
    "            # nearest gaps for unmatched (how far to the closest event in the other model)\n",
    "            gaps_unmatched = nearest_gap_for_unmatched(\n",
    "                match_df.loc[~matched, \"a_date\"], other_dates\n",
    "            )\n",
    "\n",
    "            # summary row\n",
    "            rows.append({\n",
    "                \"model\": m,\n",
    "                f\"n_{event}_base\": len(base_dates),\n",
    "                f\"n_{event}_model\": len(other_dates),\n",
    "                f\"{event}_aligned_pct\": pct_aligned,\n",
    "                f\"{event}_matched_gap_median_w\": np.median(gaps_matched) if not gaps_matched.empty else np.nan,\n",
    "                f\"{event}_matched_gap_mean_w\": gaps_matched.mean() if not gaps_matched.empty else np.nan,\n",
    "                f\"{event}_unmatched_count\": int((~matched).sum()),\n",
    "                f\"{event}_unmatched_nearest_gap_median_w\": np.median(gaps_unmatched) if not gaps_unmatched.empty else np.nan,\n",
    "                f\"{event}_unmatched_nearest_gap_mean_w\": gaps_unmatched.mean() if not gaps_unmatched.empty else np.nan,\n",
    "            })\n",
    "\n",
    "            # dump detailed matches to CSV per model/event\n",
    "            det = match_df.copy()\n",
    "            det.columns = [f\"{event}_base\", f\"{event}_model\", f\"{event}_gap_weeks\"]\n",
    "            det.to_csv(OUTDIR / f\"detail_{event}_matches_{m}_vs_{base}.csv\", index=False)\n",
    "\n",
    "        return pd.DataFrame(rows).sort_values(\"model\")\n",
    "\n",
    "    entry_df = summarize_event_alignment(\"entry\")\n",
    "    exit_df  = summarize_event_alignment(\"exit\")\n",
    "\n",
    "    entry_df.to_csv(OUTDIR / \"event_alignment_entry_vs_HMM_LSTM.csv\", index=False)\n",
    "    exit_df.to_csv(OUTDIR / \"event_alignment_exit_vs_HMM_LSTM.csv\", index=False)\n",
    "\n",
    "    # ---- Print concise console views ----\n",
    "    print(\"\\n=== Weekly Overlap vs HMM_LSTM (grid: {}, window={}w) ===\".format(WEEK_FREQ, MATCH_WINDOW_WEEKS))\n",
    "    print(weekly_df.to_string(index=False, float_format=lambda x: f\"{x:,.2f}\"))\n",
    "\n",
    "    print(\"\\n=== Entry Alignment vs HMM_LSTM (±{}w) ===\".format(MATCH_WINDOW_WEEKS))\n",
    "    print(entry_df.to_string(index=False, float_format=lambda x: f\"{x:,.2f}\"))\n",
    "\n",
    "    print(\"\\n=== Exit Alignment vs HMM_LSTM (±{}w) ===\".format(MATCH_WINDOW_WEEKS))\n",
    "    print(exit_df.to_string(index=False, float_format=lambda x: f\"{x:,.2f}\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
